{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "64XjuioTwCJb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Tuple\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text: str) -> List[str]:\n",
        "    return text.lower().split()\n",
        "\n",
        "\n",
        "def get_ngrams(tokens: List[str], n: int) -> Counter:\n",
        "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1))\n",
        "\n",
        "\n",
        "def sari_score(source: str, prediction: str, references: List[str]) -> Tuple[float, dict]:\n",
        "    src_tokens = tokenize(source)\n",
        "    pred_tokens = tokenize(prediction)\n",
        "    ref_tokens_list = [tokenize(ref) for ref in references]\n",
        "\n",
        "    keep_scores = []\n",
        "    del_scores = []\n",
        "    add_scores = []\n",
        "\n",
        "    for n in range(1, 5):\n",
        "        src_ngrams = get_ngrams(src_tokens, n)\n",
        "        pred_ngrams = get_ngrams(pred_tokens, n)\n",
        "        ref_ngrams_list = [get_ngrams(ref_tokens, n) for ref_tokens in ref_tokens_list]\n",
        "\n",
        "        src_in_refs = Counter()\n",
        "        for ngram in src_ngrams:\n",
        "            count = sum(1 for ref_ngrams in ref_ngrams_list if ngram in ref_ngrams)\n",
        "            if count > 0:\n",
        "                src_in_refs[ngram] = count\n",
        "\n",
        "        kept_ngrams = Counter()\n",
        "        for ngram in pred_ngrams:\n",
        "            if ngram in src_ngrams:\n",
        "                kept_ngrams[ngram] = min(pred_ngrams[ngram], src_ngrams[ngram])\n",
        "\n",
        "        keep_prec_num = sum(min(kept_ngrams[ng], 1) for ng in kept_ngrams if ng in src_in_refs)\n",
        "        keep_prec_den = sum(kept_ngrams.values())\n",
        "        keep_prec = keep_prec_num / keep_prec_den if keep_prec_den > 0 else 0\n",
        "\n",
        "        keep_rec_num = sum(min(kept_ngrams[ng], 1) for ng in src_in_refs if ng in kept_ngrams)\n",
        "        keep_rec_den = len(src_in_refs)\n",
        "        keep_rec = keep_rec_num / keep_rec_den if keep_rec_den > 0 else 0\n",
        "\n",
        "        keep_f1 = 2 * keep_prec * keep_rec / (keep_prec + keep_rec) if (keep_prec + keep_rec) > 0 else 0\n",
        "        keep_scores.append(keep_f1)\n",
        "\n",
        "        src_not_in_refs = set(ng for ng in src_ngrams if ng not in src_in_refs)\n",
        "        deleted_ngrams = set(ng for ng in src_ngrams if ng not in pred_ngrams)\n",
        "\n",
        "        del_prec_num = len(deleted_ngrams & src_not_in_refs)\n",
        "        del_prec_den = len(deleted_ngrams)\n",
        "        del_prec = del_prec_num / del_prec_den if del_prec_den > 0 else 0\n",
        "\n",
        "        del_rec_num = len(deleted_ngrams & src_not_in_refs)\n",
        "        del_rec_den = len(src_not_in_refs)\n",
        "        del_rec = del_rec_num / del_rec_den if del_rec_den > 0 else 0\n",
        "\n",
        "        del_f1 = 2 * del_prec * del_rec / (del_prec + del_rec) if (del_prec + del_rec) > 0 else 0\n",
        "        del_scores.append(del_f1)\n",
        "\n",
        "        added_ngrams = set(ng for ng in pred_ngrams if ng not in src_ngrams)\n",
        "\n",
        "        ref_not_in_src = set()\n",
        "        for ref_ngrams in ref_ngrams_list:\n",
        "            ref_not_in_src.update(ng for ng in ref_ngrams if ng not in src_ngrams)\n",
        "\n",
        "        add_prec_num = len(added_ngrams & ref_not_in_src)\n",
        "        add_prec_den = len(added_ngrams)\n",
        "        add_prec = add_prec_num / add_prec_den if add_prec_den > 0 else 0\n",
        "\n",
        "        add_scores.append(add_prec)\n",
        "\n",
        "    keep_avg = sum(keep_scores) / 4\n",
        "    del_avg = sum(del_scores) / 4\n",
        "    add_avg = sum(add_scores) / 4\n",
        "\n",
        "    sari = (keep_avg + del_avg + add_avg) / 3 * 100\n",
        "\n",
        "    components = {\n",
        "        'keep': keep_avg * 100,\n",
        "        'delete': del_avg * 100,\n",
        "        'add': add_avg * 100\n",
        "    }\n",
        "\n",
        "    return sari, components"
      ],
      "metadata": {
        "id": "dqR2iIVdwneB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, sources: List[str], targets: List[str], tokenizer, max_length: int = 128):\n",
        "        self.sources = sources\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sources)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source = \"simplify: \" + self.sources[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        source_enc = self.tokenizer(\n",
        "            source,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        target_enc = self.tokenizer(\n",
        "            target,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        labels = target_enc['input_ids'].squeeze()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': source_enc['input_ids'].squeeze(),\n",
        "            'attention_mask': source_enc['attention_mask'].squeeze(),\n",
        "            'labels': labels\n",
        "        }"
      ],
      "metadata": {
        "id": "r0EMqk6JwLq_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_sources: List[str],\n",
        "    train_targets: List[str],\n",
        "    val_sources: List[str],\n",
        "    val_targets: List[str],\n",
        "    output_dir: str,\n",
        "    epochs: int = 3,\n",
        "    batch_size: int = 16,\n",
        "    learning_rate: float = 3e-5,\n",
        "    device: str = 'cuda'\n",
        "):\n",
        "    train_dataset = SimplificationDataset(train_sources, train_targets, tokenizer)\n",
        "    val_dataset = SimplificationDataset(val_sources, val_targets, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                val_loss += outputs.loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            model.save_pretrained(output_dir)\n",
        "            tokenizer.save_pretrained(output_dir)\n",
        "            print(f\"Saved best model to {output_dir}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "V7YhfMDJwOpP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strong_baseline(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    max_length: int = 128,\n",
        "    num_beams: int = 4,\n",
        "    device: str = 'cuda'\n",
        ") -> str:\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_text = \"simplify: \" + source\n",
        "        input_enc = tokenizer(\n",
        "            input_text,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_enc['input_ids'],\n",
        "            attention_mask=input_enc['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "KM2okXDlwWUq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 't5-small'\n",
        "output_dir = './t5-simplification'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 20\n",
        "batch_size = 16\n",
        "learning_rate = 3e-5\n",
        "\n",
        "splits = {\n",
        "    'train': 'wiki.full.aner.ori.train.95.tsv',\n",
        "    'validation': 'wiki.full.aner.ori.valid.95.tsv',\n",
        "    'test': 'wiki.full.aner.ori.test.95.tsv'\n",
        "}\n",
        "\n",
        "print(\"Loading WikiLarge dataset...\")\n",
        "train = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"train\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "val = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"validation\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "test = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"test\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "model = train_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_sources=train['Normal'].tolist(),\n",
        "    train_targets=train['Simple'].tolist(),\n",
        "    val_sources=val['Normal'].tolist(),\n",
        "    val_targets=val['Simple'].tolist(),\n",
        "    output_dir=output_dir,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "outputs = []\n",
        "for source in tqdm(test['Normal'], desc=\"Generating simplifications\"):\n",
        "    output = strong_baseline(model, tokenizer, source, device=device)\n",
        "    outputs.append(output)\n",
        "\n",
        "\n",
        "sari_scores = []\n",
        "keep_scores = []\n",
        "del_scores = []\n",
        "add_scores = []\n",
        "\n",
        "for output, source, reference in zip(outputs, test['Normal'], test['Simple']):\n",
        "    sari, components = sari_score(source, output, [reference])\n",
        "    sari_scores.append(sari)\n",
        "    keep_scores.append(components['keep'])\n",
        "    del_scores.append(components['delete'])\n",
        "    add_scores.append(components['add'])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Strong Baseline SARI Score Results\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Number of samples: {len(sari_scores)}\")\n",
        "print()\n",
        "print(f\"  SARI:        {sum(sari_scores) / len(sari_scores):.2f}\")\n",
        "print(f\"    - Keep:    {sum(keep_scores) / len(keep_scores):.2f}\")\n",
        "print(f\"    - Delete:  {sum(del_scores) / len(del_scores):.2f}\")\n",
        "print(f\"    - Add:     {sum(add_scores) / len(add_scores):.2f}\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6nvsoCwwXGV",
        "outputId": "911d7348-8d41-4a95-c77d-35347f93595a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WikiLarge dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 9303/9303 [12:01<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 1.7627, Val Loss = 1.4882\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss = 1.5156, Val Loss = 1.4143\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 9303/9303 [11:59<00:00, 12.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss = 1.4465, Val Loss = 1.3770\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 9303/9303 [12:05<00:00, 12.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss = 1.4079, Val Loss = 1.3585\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 9303/9303 [11:44<00:00, 13.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss = 1.3829, Val Loss = 1.3476\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss = 1.3632, Val Loss = 1.3366\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss = 1.3484, Val Loss = 1.3273\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss = 1.3346, Val Loss = 1.3245\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss = 1.3238, Val Loss = 1.3166\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 9303/9303 [12:02<00:00, 12.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss = 1.3141, Val Loss = 1.3140\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 9303/9303 [11:44<00:00, 13.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train Loss = 1.3058, Val Loss = 1.3094\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train Loss = 1.2981, Val Loss = 1.3060\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 9303/9303 [11:58<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train Loss = 1.2920, Val Loss = 1.3048\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 9303/9303 [11:58<00:00, 12.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train Loss = 1.2872, Val Loss = 1.3035\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train Loss = 1.2822, Val Loss = 1.3029\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 9303/9303 [11:59<00:00, 12.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Train Loss = 1.2774, Val Loss = 1.3003\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 9303/9303 [11:45<00:00, 13.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Train Loss = 1.2746, Val Loss = 1.2992\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 9303/9303 [11:59<00:00, 12.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Train Loss = 1.2715, Val Loss = 1.2991\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 9303/9303 [11:59<00:00, 12.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Train Loss = 1.2699, Val Loss = 1.2992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 9303/9303 [11:58<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train Loss = 1.2688, Val Loss = 1.2985\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating simplifications: 100%|██████████| 191/191 [01:03<00:00,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Strong Baseline SARI Score Results\n",
            "==================================================\n",
            "Number of samples: 191\n",
            "\n",
            "  SARI:        33.43\n",
            "    - Keep:    59.90\n",
            "    - Delete:  31.98\n",
            "    - Add:     8.41\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r t5-simplification.zip /content/t5-simplification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXo8Mp_fo8yZ",
        "outputId": "bc43a990-7af4-4e4a-b9c4-4598183a8633"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/t5-simplification/ (stored 0%)\n",
            "  adding: content/t5-simplification/model.safetensors (deflated 8%)\n",
            "  adding: content/t5-simplification/spiece.model (deflated 48%)\n",
            "  adding: content/t5-simplification/generation_config.json (deflated 27%)\n",
            "  adding: content/t5-simplification/special_tokens_map.json (deflated 85%)\n",
            "  adding: content/t5-simplification/tokenizer.json (deflated 74%)\n",
            "  adding: content/t5-simplification/tokenizer_config.json (deflated 95%)\n",
            "  adding: content/t5-simplification/config.json (deflated 63%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('t5-simplification.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KLfzLGIgpBN3",
        "outputId": "98ee885a-2256-41ff-969d-8cf4babbeb12"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d3ab1e98-e594-425e-9438-2b1342e2254d\", \"t5-simplification.zip\", 224818645)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aec905c"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('t5-simplification.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "output_dir = 'content/t5-simplification'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = T5ForConditionalGeneration.from_pretrained(output_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model.to(device)\n",
        "\n",
        "# Load test data\n",
        "test = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/wiki.full.aner.ori.test.95.tsv\",\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "\n",
        "# Generate predictions and calculate SARI scores for each example\n",
        "print(\"Generating predictions and calculating scores...\")\n",
        "results = []\n",
        "for idx, row in tqdm(test.iterrows(), total=len(test)):\n",
        "    source = row['Normal']\n",
        "    reference = row['Simple']\n",
        "\n",
        "    # Generate simplification\n",
        "    output = strong_baseline(model, tokenizer, source, device=device)\n",
        "\n",
        "    # Calculate SARI score\n",
        "    sari, components = sari_score(source, output, [reference])\n",
        "\n",
        "    results.append({\n",
        "        'idx': idx,\n",
        "        'source': source,\n",
        "        'reference': reference,\n",
        "        'prediction': output,\n",
        "        'sari': sari,\n",
        "        'keep': components['keep'],\n",
        "        'delete': components['delete'],\n",
        "        'add': components['add']\n",
        "    })\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Sort by SARI score\n",
        "results_df = results_df.sort_values('sari', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display top 5 examples (highest SARI scores)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 5 EXAMPLES (Highest SARI Scores)\")\n",
        "print(\"=\"*80)\n",
        "for i, row in results_df.head(5).iterrows():\n",
        "    print(f\"\\nExample {i+1} - SARI: {row['sari']:.2f} (Keep: {row['keep']:.2f}, Del: {row['delete']:.2f}, Add: {row['add']:.2f})\")\n",
        "    print(f\"Source:     {row['source']}\")\n",
        "    print(f\"Reference:  {row['reference']}\")\n",
        "    print(f\"Prediction: {row['prediction']}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "# Display bottom 5 examples (lowest SARI scores)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BOTTOM 5 EXAMPLES (Lowest SARI Scores)\")\n",
        "print(\"=\"*80)\n",
        "for i, row in results_df.tail(5).iterrows():\n",
        "    print(f\"\\nExample {i+1} - SARI: {row['sari']:.2f} (Keep: {row['keep']:.2f}, Del: {row['delete']:.2f}, Add: {row['add']:.2f})\")\n",
        "    print(f\"Source:     {row['source']}\")\n",
        "    print(f\"Reference:  {row['reference']}\")\n",
        "    print(f\"Prediction: {row['prediction']}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total examples: {len(results_df)}\")\n",
        "print(f\"\\nSARI Score Distribution:\")\n",
        "print(f\"  Mean:   {results_df['sari'].mean():.2f}\")\n",
        "print(f\"  Median: {results_df['sari'].median():.2f}\")\n",
        "print(f\"  Std:    {results_df['sari'].std():.2f}\")\n",
        "print(f\"  Min:    {results_df['sari'].min():.2f}\")\n",
        "print(f\"  Max:    {results_df['sari'].max():.2f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save results to CSV for further analysis\n",
        "results_df.to_csv('model_analysis_results.csv', index=False)\n",
        "print(f\"\\nResults saved to 'model_analysis_results.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "bKZrQw59ozY-",
        "outputId": "370fe7a0-717d-446f-e57f-4d9c81556ef9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions and calculating scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 14/191 [00:04<01:00,  2.91it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-970059827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Generate simplification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrong_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Calculate SARI score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2616694279.py\u001b[0m in \u001b[0;36mstrong_baseline\u001b[0;34m(model, tokenizer, source, max_length, num_beams, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m         ).to(device)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         output_ids = model.generate(\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3348\u001b[0m             \u001b[0;31m# f. Update the completed beams if a new high score in a finished sequence is found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3349\u001b[0;31m             sequences, beam_scores, beam_indices, is_sent_finished = self._update_finished_beams(\n\u001b[0m\u001b[1;32m   3350\u001b[0m                 \u001b[0msequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3351\u001b[0m                 \u001b[0mtopk_running_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopk_running_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_update_finished_beams\u001b[0;34m(self, sequences, topk_running_sequences, beam_scores, topk_log_probs, beam_indices, topk_running_beam_indices, is_early_stop_heuristic_unsatisfied, is_sent_finished, next_token_hits_stopping_criteria, top_num_beam_mask, num_beams, cur_len, decoder_prompt_len, length_penalty, early_stopping)\u001b[0m\n\u001b[1;32m   3088\u001b[0m         \u001b[0mtopk_log_probs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbeams_in_batch_are_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0e9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3089\u001b[0m         \u001b[0;31m# - make sure no scores can be added anymore if improvement is not possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3090\u001b[0;31m         \u001b[0mtopk_log_probs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mis_early_stop_heuristic_unsatisfied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0e9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3092\u001b[0m         \u001b[0;31m# - make sure still running sequences cannot be chosen as finalized beam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}