{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a343d16a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import subprocess\n",
        "from typing import List, Dict\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, T5ForConditionalGeneration\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eda40f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prefer local `data/test.csv` (no network needed). Fallback: load WikiLarge from HF.\n",
        "\n",
        "FORCE_HF_TEST = False  # set True to ignore local data/ and use the full HF test split\n",
        "\n",
        "DATA_TEST_PATH = os.path.join(\"data\", \"test.csv\")\n",
        "\n",
        "if (not FORCE_HF_TEST) and os.path.exists(DATA_TEST_PATH):\n",
        "    test = pd.read_csv(DATA_TEST_PATH)\n",
        "    missing = {\"Normal\", \"Simple\"} - set(test.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"{DATA_TEST_PATH} missing columns: {missing}\")\n",
        "\n",
        "    print(f\"Loaded local test split: {DATA_TEST_PATH} ({len(test)} rows)\")\n",
        "    if len(test) <= 1000:\n",
        "        print(\n",
        "            \"WARNING: This is a small subset test file. \"\n",
        "            \"Set FORCE_HF_TEST=True (and ensure Colab has network) to evaluate on the full WikiLarge test split.\"\n",
        "        )\n",
        "else:\n",
        "    splits = {\n",
        "        \"train\": \"wiki.full.aner.ori.train.95.tsv\",\n",
        "        \"validation\": \"wiki.full.aner.ori.valid.95.tsv\",\n",
        "        \"test\": \"wiki.full.aner.ori.test.95.tsv\",\n",
        "    }\n",
        "    test = pd.read_csv(\n",
        "        \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"test\"],\n",
        "        sep=\"\\t\",\n",
        "    )\n",
        "    print(f\"Loaded HF test split ({len(test)} rows)\")\n",
        "\n",
        "test_sources = test[\"Normal\"].astype(str).tolist()\n",
        "test_refs = test[\"Simple\"].astype(str).tolist()\n",
        "print(\"Example source:\", test_sources[0][:200])\n",
        "print(\"Example ref:\", test_refs[0][:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d1d212",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Unzip / prepare trained model folders ----\n",
        "\n",
        "def ensure_dir_from_zip(zip_path: str, out_dir: str) -> None:\n",
        "    \"\"\"Ensure `out_dir` exists by extracting `zip_path` if needed.\"\"\"\n",
        "    if os.path.isdir(out_dir):\n",
        "        return\n",
        "    if not os.path.exists(zip_path):\n",
        "        raise FileNotFoundError(f\"Missing {zip_path} and {out_dir} does not exist.\")\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    print(f\"Extracting {zip_path} -> {out_dir} ...\")\n",
        "    shutil.unpack_archive(zip_path, out_dir)\n",
        "\n",
        "\n",
        "def maybe_flatten_nested_dir(out_dir: str, nested_name: str, required_markers: List[str]) -> None:\n",
        "    \"\"\"Handle zips that extract to out_dir/nested_name/* instead of out_dir/*.\"\"\"\n",
        "    if all(os.path.exists(os.path.join(out_dir, m)) for m in required_markers):\n",
        "        return\n",
        "\n",
        "    nested = os.path.join(out_dir, nested_name)\n",
        "    if not os.path.isdir(nested):\n",
        "        return\n",
        "\n",
        "    if not all(os.path.exists(os.path.join(nested, m)) for m in required_markers):\n",
        "        return\n",
        "\n",
        "    print(f\"Flattening nested extraction: {nested} -> {out_dir}\")\n",
        "    for name in os.listdir(nested):\n",
        "        shutil.move(os.path.join(nested, name), os.path.join(out_dir, name))\n",
        "    shutil.rmtree(nested)\n",
        "\n",
        "\n",
        "# Baseline checkpoint\n",
        "BASELINE_DIR = \"t5-simplification\"\n",
        "BASELINE_ZIP = \"t5-simplification.zip\"\n",
        "\n",
        "# Extension 3 checkpoint (encoder-adapter architecture)\n",
        "EXT3_DIR = \"t5-simplification-2\"\n",
        "EXT3_ZIP = \"t5-simplification-2.zip\"\n",
        "\n",
        "# Baseline zip may be archived with a 'content/t5-simplification/' prefix.\n",
        "if not os.path.isdir(BASELINE_DIR):\n",
        "    tmp_extract = \"_tmp_extract_baseline\"\n",
        "    if os.path.isdir(tmp_extract):\n",
        "        shutil.rmtree(tmp_extract)\n",
        "    os.makedirs(tmp_extract, exist_ok=True)\n",
        "\n",
        "    print(f\"Extracting {BASELINE_ZIP} -> {tmp_extract} ...\")\n",
        "    shutil.unpack_archive(BASELINE_ZIP, tmp_extract)\n",
        "\n",
        "    candidate = os.path.join(tmp_extract, \"content\", \"t5-simplification\")\n",
        "    if os.path.isdir(candidate):\n",
        "        shutil.move(candidate, BASELINE_DIR)\n",
        "        shutil.rmtree(tmp_extract)\n",
        "    else:\n",
        "        # If the zip already contains the folder at root, extract directly.\n",
        "        shutil.rmtree(tmp_extract)\n",
        "        ensure_dir_from_zip(BASELINE_ZIP, BASELINE_DIR)\n",
        "\n",
        "# Ext3 is expected to extract cleanly, but keep it robust.\n",
        "ensure_dir_from_zip(EXT3_ZIP, EXT3_DIR)\n",
        "\n",
        "maybe_flatten_nested_dir(BASELINE_DIR, BASELINE_DIR, required_markers=[\"config.json\"])\n",
        "maybe_flatten_nested_dir(EXT3_DIR, EXT3_DIR, required_markers=[\"adapter_config.json\", \"adapter.pt\"])\n",
        "\n",
        "print(\"Baseline dir ok:\", os.path.isdir(BASELINE_DIR))\n",
        "print(\"Ext3 dir ok:\", os.path.isdir(EXT3_DIR))\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99dfc00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Load baseline model (plain T5 checkpoint) ----\n",
        "\n",
        "print(\"Loading baseline model...\")\n",
        "baseline_tokenizer = AutoTokenizer.from_pretrained(BASELINE_DIR, local_files_only=True)\n",
        "baseline_model = T5ForConditionalGeneration.from_pretrained(BASELINE_DIR, local_files_only=True)\n",
        "baseline_model.to(DEVICE).eval()\n",
        "print(\"Loaded baseline.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd83595",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Load Extension 3 model (T5 + encoder adapter) ----\n",
        "\n",
        "class EncoderBottleneckAdapter(nn.Module):\n",
        "    def __init__(self, d_model: int, bottleneck: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.down = nn.Linear(d_model, bottleneck)\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.up = nn.Linear(bottleneck, d_model)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        # Trainable scalar gate initialized near 0 (start close to plain T5)\n",
        "        self.gate = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, h: torch.Tensor) -> torch.Tensor:\n",
        "        delta = self.up(self.dropout(self.act(self.down(h))))\n",
        "        return self.ln(h + torch.tanh(self.gate) * delta)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AdapterConfig:\n",
        "    bottleneck: int\n",
        "    dropout: float\n",
        "\n",
        "\n",
        "class T5WithEncoderAdapter(nn.Module):\n",
        "    def __init__(self, base_model_name: str, bottleneck: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        config = AutoConfig.from_pretrained(base_model_name, local_files_only=True)\n",
        "        self.base = T5ForConditionalGeneration.from_pretrained(\n",
        "            base_model_name,\n",
        "            config=config,\n",
        "            local_files_only=True,\n",
        "        )\n",
        "        d_model = self.base.config.d_model\n",
        "        self.adapter = EncoderBottleneckAdapter(d_model=d_model, bottleneck=bottleneck, dropout=dropout)\n",
        "        self.adapter_cfg = AdapterConfig(bottleneck=bottleneck, dropout=dropout)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
        "        enc = self.base.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        h = self.adapter(enc.last_hidden_state)\n",
        "        encoder_outputs = BaseModelOutput(\n",
        "            last_hidden_state=h,\n",
        "            hidden_states=enc.hidden_states,\n",
        "            attentions=enc.attentions,\n",
        "        )\n",
        "        return self.base(\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            return_dict=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids=None, attention_mask=None, **gen_kwargs):\n",
        "        enc = self.base.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        h = self.adapter(enc.last_hidden_state)\n",
        "        encoder_outputs = BaseModelOutput(\n",
        "            last_hidden_state=h,\n",
        "            hidden_states=enc.hidden_states,\n",
        "            attentions=enc.attentions,\n",
        "        )\n",
        "        return self.base.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            **gen_kwargs,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, output_dir: str):\n",
        "        base_dir = os.path.join(output_dir, \"base\")\n",
        "        with open(os.path.join(output_dir, \"adapter_config.json\"), \"r\") as f:\n",
        "            cfg = json.load(f)\n",
        "        model = cls(base_model_name=base_dir, bottleneck=cfg[\"bottleneck\"], dropout=cfg[\"dropout\"])\n",
        "        sd = torch.load(os.path.join(output_dir, \"adapter.pt\"), map_location=\"cpu\")\n",
        "        model.adapter.load_state_dict(sd)\n",
        "        return model\n",
        "\n",
        "\n",
        "print(\"Loading Extension 3 model...\")\n",
        "ext3_tokenizer = AutoTokenizer.from_pretrained(EXT3_DIR, local_files_only=True)\n",
        "ext3_model = T5WithEncoderAdapter.load(EXT3_DIR)\n",
        "ext3_model.to(DEVICE).eval()\n",
        "print(\"Loaded Extension 3.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1fb5d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Completion-ratio helpers + batched generation ----\n",
        "\n",
        "COMPLETION_RATIOS = [0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "\n",
        "def get_sentence_prefix(source: str, completion_ratio: float) -> str:\n",
        "    tokens = str(source).split()\n",
        "    num_tokens = len(tokens)\n",
        "    prefix_length = max(1, int(num_tokens * completion_ratio))\n",
        "    return \" \".join(tokens[:prefix_length])\n",
        "\n",
        "\n",
        "def generate_predictions(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    sources: List[str],\n",
        "    *,\n",
        "    completion_ratio: float,\n",
        "    batch_size: int = 16,\n",
        "    max_length: int = 128,\n",
        "    num_beams: int = 4,\n",
        ") -> List[str]:\n",
        "    \"\"\"Generate predictions using prefix-based incremental inputs.\"\"\"\n",
        "    preds: List[str] = []\n",
        "\n",
        "    model.eval()\n",
        "    for i in tqdm(range(0, len(sources), batch_size), desc=f\"Generating ({int(completion_ratio*100)}%)\"):\n",
        "        batch = sources[i : i + batch_size]\n",
        "        inputs = [\"simplify: \" + get_sentence_prefix(s, completion_ratio) for s in batch]\n",
        "\n",
        "        enc = tokenizer(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out_ids = model.generate(\n",
        "                input_ids=enc[\"input_ids\"],\n",
        "                attention_mask=enc[\"attention_mask\"],\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "\n",
        "        texts = tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n",
        "        # Ensure one-line outputs for scoring scripts\n",
        "        texts = [t.replace(\"\\n\", \" \").strip() for t in texts]\n",
        "        preds.extend(texts)\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b98b425",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Run both models on the test set (25/50/75/100% completion) ----\n",
        "\n",
        "baseline_preds_by_ratio: Dict[float, List[str]] = {}\n",
        "ext3_preds_by_ratio: Dict[float, List[str]] = {}\n",
        "\n",
        "for r in COMPLETION_RATIOS:\n",
        "    baseline_preds_by_ratio[r] = generate_predictions(\n",
        "        baseline_model,\n",
        "        baseline_tokenizer,\n",
        "        test_sources,\n",
        "        completion_ratio=r,\n",
        "        batch_size=16,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "    )\n",
        "    ext3_preds_by_ratio[r] = generate_predictions(\n",
        "        ext3_model,\n",
        "        ext3_tokenizer,\n",
        "        test_sources,\n",
        "        completion_ratio=r,\n",
        "        batch_size=16,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "    )\n",
        "\n",
        "    assert len(baseline_preds_by_ratio[r]) == len(test_sources)\n",
        "    assert len(ext3_preds_by_ratio[r]) == len(test_sources)\n",
        "\n",
        "    print(f\"Sample baseline pred ({int(r*100)}%):\", baseline_preds_by_ratio[r][0][:200])\n",
        "    print(f\"Sample ext3 pred ({int(r*100)}%):\", ext3_preds_by_ratio[r][0][:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ec5bcd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Write `output/` bundle (predictions + gold labels + README) ----\n",
        "\n",
        "OUTPUT_DIR = \"output\"\n",
        "BASELINE_OUT_DIR = os.path.join(OUTPUT_DIR, BASELINE_DIR)\n",
        "EXT3_OUT_DIR = os.path.join(OUTPUT_DIR, EXT3_DIR)\n",
        "\n",
        "os.makedirs(BASELINE_OUT_DIR, exist_ok=True)\n",
        "os.makedirs(EXT3_OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Note: for SARI (and to match the repoâ€™s existing evaluation), `sources.txt` is ALWAYS the full source,\n",
        "# even when the model input is a prefix. Only the model predictions vary by completion ratio.\n",
        "sources_path = os.path.join(OUTPUT_DIR, \"sources.txt\")\n",
        "refs_path = os.path.join(OUTPUT_DIR, \"references.txt\")\n",
        "\n",
        "with open(sources_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(test_sources) + \"\\n\")\n",
        "\n",
        "with open(refs_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(test_refs) + \"\\n\")\n",
        "\n",
        "print(\"Wrote:\")\n",
        "print(\"-\", sources_path)\n",
        "print(\"-\", refs_path)\n",
        "\n",
        "# Write per-ratio predictions + aligned TSVs\n",
        "baseline_preds_paths: Dict[float, str] = {}\n",
        "ext3_preds_paths: Dict[float, str] = {}\n",
        "\n",
        "for r in COMPLETION_RATIOS:\n",
        "    pct = int(r * 100)\n",
        "    baseline_ratio_dir = os.path.join(BASELINE_OUT_DIR, f\"{pct}\")\n",
        "    ext3_ratio_dir = os.path.join(EXT3_OUT_DIR, f\"{pct}\")\n",
        "    os.makedirs(baseline_ratio_dir, exist_ok=True)\n",
        "    os.makedirs(ext3_ratio_dir, exist_ok=True)\n",
        "\n",
        "    baseline_preds_path = os.path.join(baseline_ratio_dir, \"predictions.txt\")\n",
        "    ext3_preds_path = os.path.join(ext3_ratio_dir, \"predictions.txt\")\n",
        "\n",
        "    with open(baseline_preds_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(baseline_preds_by_ratio[r]) + \"\\n\")\n",
        "\n",
        "    with open(ext3_preds_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(ext3_preds_by_ratio[r]) + \"\\n\")\n",
        "\n",
        "    baseline_preds_paths[r] = baseline_preds_path\n",
        "    ext3_preds_paths[r] = ext3_preds_path\n",
        "\n",
        "    aligned_tsv = os.path.join(OUTPUT_DIR, f\"aligned_{pct}.tsv\")\n",
        "    pd.DataFrame(\n",
        "        {\n",
        "            \"source\": test_sources,\n",
        "            \"reference\": test_refs,\n",
        "            \"pred_t5_simplification\": baseline_preds_by_ratio[r],\n",
        "            \"pred_t5_simplification_2\": ext3_preds_by_ratio[r],\n",
        "        }\n",
        "    ).to_csv(aligned_tsv, sep=\"\\t\", index=False)\n",
        "\n",
        "    print(f\"- {baseline_preds_path}\")\n",
        "    print(f\"- {ext3_preds_path}\")\n",
        "    print(f\"- {aligned_tsv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1039f4dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Zip outputs for easy download ----\n",
        "\n",
        "zip_path = shutil.make_archive(\"output\", \"zip\", root_dir=OUTPUT_DIR)\n",
        "print(\"Created:\", zip_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
