{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2095dcda",
      "metadata": {},
      "source": [
        "# Model Comparison: Strong Baseline vs Extensions 1–3\n",
        "\n",
        "This notebook compares **Strong Baseline**, **Extension 1**, **Extension 2**, and **Extension 3** on:\n",
        "\n",
        "- **SARI** (plus keep/delete/add components)\n",
        "- **Latency** (ms per example)\n",
        "\n",
        "It also evaluates a 5th setting:\n",
        "\n",
        "- **Extension 3 + beam-techniques**: applying Extension 2’s adaptive decoding + context-aware length penalty to the Extension 3 architecture.\n",
        "\n",
        "All comparisons are run across completion ratios **25%, 50%, 75%, 100%** (prefix-based incremental inputs), matching the setup used in `extension2.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a10d6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import AutoConfig, AutoTokenizer, T5ForConditionalGeneration\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "from score import sari_score\n",
        "\n",
        "# Repro / display\n",
        "np.random.seed(5300)\n",
        "plt.rcParams[\"figure.dpi\"] = 120\n",
        "\n",
        "# Runtime config\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_LENGTH = 128\n",
        "COMPLETION_RATIOS = [0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "EVAL_N: int | None = None\n",
        "\n",
        "print(\"DEVICE:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b84b37",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "# - Prefer local `data/test.csv` (offline-friendly)\n",
        "# - Fallback to HF WikiLarge test split if local data isn't present\n",
        "\n",
        "FORCE_HF_TEST = False  # set True to ignore local data/ and use the HF test split\n",
        "\n",
        "LOCAL_TEST_CANDIDATES = [\n",
        "    os.path.join(\"data\", \"test.csv\"),\n",
        "    os.path.join(\"..\", \"data\", \"test.csv\"),  # if running from within code/\n",
        "]\n",
        "\n",
        "TEST_PATH = next((p for p in LOCAL_TEST_CANDIDATES if os.path.exists(p)), None)\n",
        "\n",
        "if (not FORCE_HF_TEST) and TEST_PATH is not None:\n",
        "    test_df = pd.read_csv(TEST_PATH)\n",
        "    print(f\"Loaded local test split: {TEST_PATH} ({len(test_df)} rows)\")\n",
        "else:\n",
        "    splits = {\n",
        "        \"test\": \"wiki.full.aner.ori.test.95.tsv\",\n",
        "    }\n",
        "    test_df = pd.read_csv(\n",
        "        \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"test\"],\n",
        "        sep=\"\\t\",\n",
        "    )\n",
        "    print(f\"Loaded HF test split ({len(test_df)} rows)\")\n",
        "\n",
        "if EVAL_N is not None:\n",
        "    test_df = test_df.head(EVAL_N).reset_index(drop=True)\n",
        "    print(f\"Subsampled to EVAL_N={EVAL_N} rows\")\n",
        "\n",
        "required_cols = {\"Normal\", \"Simple\"}\n",
        "missing = required_cols - set(test_df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"test_df missing columns: {missing}\")\n",
        "\n",
        "print(\"Loaded test examples:\", len(test_df))\n",
        "print(test_df.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114a90d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_dir_from_zip(zip_path: str, out_dir: str) -> None:\n",
        "    \"\"\"Ensure `out_dir` exists by extracting `zip_path` if needed.\"\"\"\n",
        "    if os.path.isdir(out_dir):\n",
        "        return\n",
        "    if not os.path.exists(zip_path):\n",
        "        raise FileNotFoundError(f\"Missing {zip_path} and {out_dir} does not exist.\")\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    print(f\"Extracting {zip_path} -> {out_dir} ...\")\n",
        "    shutil.unpack_archive(zip_path, out_dir)\n",
        "\n",
        "\n",
        "# Strong baseline checkpoint\n",
        "BASELINE_DIR = \"t5-simplification\"\n",
        "BASELINE_ZIP = \"t5-simplification.zip\"\n",
        "\n",
        "# Extension 3 checkpoint (encoder-adapter architecture)\n",
        "EXT3_DIR = \"t5-simplification-2\"\n",
        "EXT3_ZIP = \"t5-simplification-2.zip\"\n",
        "\n",
        "# Baseline is already present in this repo in most cases; keep this robust anyway.\n",
        "if not os.path.isdir(BASELINE_DIR):\n",
        "    # The baseline zip in this repo is archived with a 'content/t5-simplification/' prefix.\n",
        "    # We extract to a temp directory and then move the actual folder into place.\n",
        "    tmp_extract = \"_tmp_extract_baseline\"\n",
        "    if os.path.isdir(tmp_extract):\n",
        "        shutil.rmtree(tmp_extract)\n",
        "    os.makedirs(tmp_extract, exist_ok=True)\n",
        "\n",
        "    print(f\"Extracting {BASELINE_ZIP} -> {tmp_extract} ...\")\n",
        "    shutil.unpack_archive(BASELINE_ZIP, tmp_extract)\n",
        "\n",
        "    candidate = os.path.join(tmp_extract, \"content\", \"t5-simplification\")\n",
        "    if not os.path.isdir(candidate):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Expected {candidate} after extracting {BASELINE_ZIP}, but it was not found.\"\n",
        "        )\n",
        "\n",
        "    shutil.move(candidate, BASELINE_DIR)\n",
        "    shutil.rmtree(tmp_extract)\n",
        "\n",
        "# Extension 3 is only zipped in this repo snapshot\n",
        "ensure_dir_from_zip(EXT3_ZIP, EXT3_DIR)\n",
        "\n",
        "print(\"Baseline dir ok:\", os.path.isdir(BASELINE_DIR))\n",
        "print(\"Ext3 dir ok:\", os.path.isdir(EXT3_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4792f941",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- Load baseline checkpoint (shared by baseline/ext1/ext2 decoding) -----\n",
        "\n",
        "print(\"Loading baseline model...\")\n",
        "baseline_tokenizer = AutoTokenizer.from_pretrained(BASELINE_DIR, local_files_only=True)\n",
        "baseline_model = T5ForConditionalGeneration.from_pretrained(BASELINE_DIR, local_files_only=True)\n",
        "baseline_model.to(DEVICE).eval()\n",
        "print(\"Loaded baseline model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db1db64",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- Extension 3 model definition (copied/simplified from extension3.ipynb) -----\n",
        "\n",
        "class EncoderBottleneckAdapter(nn.Module):\n",
        "    def __init__(self, d_model: int, bottleneck: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.down = nn.Linear(d_model, bottleneck)\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.up = nn.Linear(bottleneck, d_model)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        # Trainable scalar gate initialized near 0: start close to plain T5\n",
        "        self.gate = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, h: torch.Tensor) -> torch.Tensor:\n",
        "        delta = self.up(self.dropout(self.act(self.down(h))))\n",
        "        return self.ln(h + torch.tanh(self.gate) * delta)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AdapterConfig:\n",
        "    bottleneck: int\n",
        "    dropout: float\n",
        "\n",
        "\n",
        "class T5WithEncoderAdapter(nn.Module):\n",
        "    def __init__(self, base_model_name: str, bottleneck: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        config = AutoConfig.from_pretrained(base_model_name, local_files_only=True)\n",
        "        self.base = T5ForConditionalGeneration.from_pretrained(\n",
        "            base_model_name,\n",
        "            config=config,\n",
        "            local_files_only=True,\n",
        "        )\n",
        "        d_model = self.base.config.d_model\n",
        "        self.adapter = EncoderBottleneckAdapter(d_model=d_model, bottleneck=bottleneck, dropout=dropout)\n",
        "        self.adapter_cfg = AdapterConfig(bottleneck=bottleneck, dropout=dropout)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
        "        enc = self.base.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        h = self.adapter(enc.last_hidden_state)\n",
        "        encoder_outputs = BaseModelOutput(\n",
        "            last_hidden_state=h,\n",
        "            hidden_states=enc.hidden_states,\n",
        "            attentions=enc.attentions,\n",
        "        )\n",
        "        return self.base(\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            return_dict=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids=None, attention_mask=None, **gen_kwargs):\n",
        "        enc = self.base.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        h = self.adapter(enc.last_hidden_state)\n",
        "        encoder_outputs = BaseModelOutput(\n",
        "            last_hidden_state=h,\n",
        "            hidden_states=enc.hidden_states,\n",
        "            attentions=enc.attentions,\n",
        "        )\n",
        "        return self.base.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            **gen_kwargs,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, output_dir: str):\n",
        "        base_dir = os.path.join(output_dir, \"base\")\n",
        "        with open(os.path.join(output_dir, \"adapter_config.json\"), \"r\") as f:\n",
        "            cfg = json.load(f)\n",
        "        model = cls(base_model_name=base_dir, bottleneck=cfg[\"bottleneck\"], dropout=cfg[\"dropout\"])\n",
        "        sd = torch.load(os.path.join(output_dir, \"adapter.pt\"), map_location=\"cpu\")\n",
        "        model.adapter.load_state_dict(sd)\n",
        "        return model\n",
        "\n",
        "\n",
        "# ----- Load Extension 3 checkpoint -----\n",
        "# Baseline / Ext1 / Ext2 numbers are hardcoded below, so we don't need to load the baseline model.\n",
        "\n",
        "print(\"Loading Extension 3 model...\")\n",
        "ext3_tokenizer = AutoTokenizer.from_pretrained(EXT3_DIR, local_files_only=True)\n",
        "ext3_model = T5WithEncoderAdapter.load(EXT3_DIR).to(DEVICE)\n",
        "ext3_model.eval()\n",
        "\n",
        "print(\"Loaded ext3 model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3aa59b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Shared helpers (from extension2.ipynb) ---\n",
        "\n",
        "def get_sentence_prefix(source: str, completion_ratio: float) -> str:\n",
        "    tokens = source.split()\n",
        "    num_tokens = len(tokens)\n",
        "    prefix_length = max(1, int(num_tokens * completion_ratio))\n",
        "    return \" \".join(tokens[:prefix_length])\n",
        "\n",
        "\n",
        "def compute_context_aware_length_penalty(source: str, completion_ratio: float) -> float:\n",
        "    source_length = len(source.split())\n",
        "\n",
        "    # Base length penalty increases with completion ratio\n",
        "    base_penalty = 0.4 + (completion_ratio * 0.4)  # Range: 0.4 to 0.8\n",
        "\n",
        "    # Adjust based on source length\n",
        "    if source_length < 10:\n",
        "        length_factor = 0.9\n",
        "    elif source_length < 20:\n",
        "        length_factor = 1.0\n",
        "    else:\n",
        "        length_factor = 1.1\n",
        "\n",
        "    final_penalty = base_penalty * length_factor\n",
        "    return max(0.3, min(1.0, final_penalty))\n",
        "\n",
        "\n",
        "def _encode(tokenizer, text: str):\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ee9962",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Decoding strategies ---\n",
        "\n",
        "def simplify_fixed(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    completion_ratio: float,\n",
        "    *,\n",
        "    num_beams: int = 4,\n",
        "    length_penalty: float = 0.6,\n",
        "    no_repeat_ngram_size: int = 2,\n",
        ") -> Tuple[str, float]:\n",
        "    \"\"\"Strong baseline-style decoding with fixed generation params.\"\"\"\n",
        "    prefix = get_sentence_prefix(source, completion_ratio)\n",
        "    enc = _encode(tokenizer, \"simplify: \" + prefix)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            input_ids=enc[\"input_ids\"],\n",
        "            attention_mask=enc[\"attention_mask\"],\n",
        "            max_length=MAX_LENGTH,\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=length_penalty,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    pred = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    return pred, (t1 - t0)\n",
        "\n",
        "\n",
        "def simplify_ext1_adaptive_lenpen(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    completion_ratio: float,\n",
        "    *,\n",
        "    num_beams: int = 4,\n",
        "    no_repeat_ngram_size: int = 2,\n",
        ") -> Tuple[str, float]:\n",
        "    \"\"\"Extension 1: adaptive length penalty based on completion ratio.\"\"\"\n",
        "    if completion_ratio <= 0.25:\n",
        "        length_penalty = 0.4\n",
        "    elif completion_ratio <= 0.5:\n",
        "        length_penalty = 0.5\n",
        "    elif completion_ratio <= 0.75:\n",
        "        length_penalty = 0.6\n",
        "    else:\n",
        "        length_penalty = 0.7\n",
        "\n",
        "    return simplify_fixed(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        source,\n",
        "        completion_ratio,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=length_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "    )\n",
        "\n",
        "\n",
        "def simplify_ext2_adaptive_decoding(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    completion_ratio: float,\n",
        "    *,\n",
        "    no_repeat_ngram_size: int = 2,\n",
        ") -> Tuple[str, float]:\n",
        "    \"\"\"Extension 2: greedy early (<=50%), beam later, context-aware length penalty.\"\"\"\n",
        "    prefix = get_sentence_prefix(source, completion_ratio)\n",
        "\n",
        "    if completion_ratio <= 0.5:\n",
        "        num_beams = 1\n",
        "    else:\n",
        "        num_beams = 4\n",
        "\n",
        "    length_penalty = compute_context_aware_length_penalty(source, completion_ratio)\n",
        "\n",
        "    enc = _encode(tokenizer, \"simplify: \" + prefix)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            input_ids=enc[\"input_ids\"],\n",
        "            attention_mask=enc[\"attention_mask\"],\n",
        "            max_length=MAX_LENGTH,\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=length_penalty,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    pred = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    return pred, (t1 - t0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3abab506",
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_over_completion_ratios(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    df: pd.DataFrame,\n",
        "    simplify_fn,\n",
        "    *,\n",
        "    label: str,\n",
        ") -> Dict[float, Dict[str, List[float]]]:\n",
        "    \"\"\"Returns results[ratio] = {'sari','keep','delete','add','latency_s'} lists.\"\"\"\n",
        "    results: Dict[float, Dict[str, List[float]]] = {\n",
        "        r: {\"sari\": [], \"keep\": [], \"delete\": [], \"add\": [], \"latency_s\": []}\n",
        "        for r in COMPLETION_RATIOS\n",
        "    }\n",
        "\n",
        "    iterator = tqdm(df.itertuples(index=False), total=len(df), desc=f\"eval: {label}\")\n",
        "    for row in iterator:\n",
        "        source = str(getattr(row, \"Normal\"))\n",
        "        reference = str(getattr(row, \"Simple\"))\n",
        "\n",
        "        for r in COMPLETION_RATIOS:\n",
        "            pred, latency_s = simplify_fn(model, tokenizer, source, r)\n",
        "            s, comps = sari_score(source, pred, [reference])\n",
        "            results[r][\"sari\"].append(float(s))\n",
        "            results[r][\"keep\"].append(float(comps[\"keep\"]))\n",
        "            results[r][\"delete\"].append(float(comps[\"delete\"]))\n",
        "            results[r][\"add\"].append(float(comps[\"add\"]))\n",
        "            results[r][\"latency_s\"].append(float(latency_s))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def mean_series(results: Dict[float, Dict[str, List[float]]], key: str) -> List[float]:\n",
        "    return [float(np.mean(results[r][key])) for r in COMPLETION_RATIOS]\n",
        "\n",
        "\n",
        "def ms_series(results: Dict[float, Dict[str, List[float]]]) -> List[float]:\n",
        "    return [float(np.mean(results[r][\"latency_s\"])) * 1000.0 for r in COMPLETION_RATIOS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8d3532",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all systems end-to-end on the loaded test set\n",
        "# (baseline weights are reused for Baseline/Ext1/Ext2; Ext3 uses its own checkpoint)\n",
        "\n",
        "SYSTEMS_TO_RUN = {\n",
        "    \"Strong Baseline (fixed)\": {\n",
        "        \"model\": baseline_model,\n",
        "        \"tokenizer\": baseline_tokenizer,\n",
        "        \"fn\": simplify_fixed,\n",
        "    },\n",
        "    \"Extension 1 (adaptive lenpen)\": {\n",
        "        \"model\": baseline_model,\n",
        "        \"tokenizer\": baseline_tokenizer,\n",
        "        \"fn\": simplify_ext1_adaptive_lenpen,\n",
        "    },\n",
        "    \"Extension 2 (adaptive decoding)\": {\n",
        "        \"model\": baseline_model,\n",
        "        \"tokenizer\": baseline_tokenizer,\n",
        "        \"fn\": simplify_ext2_adaptive_decoding,\n",
        "    },\n",
        "    \"Extension 3 (arch, fixed)\": {\n",
        "        \"model\": ext3_model,\n",
        "        \"tokenizer\": ext3_tokenizer,\n",
        "        \"fn\": simplify_fixed,\n",
        "    },\n",
        "    \"Extension 3 + beam-techniques\": {\n",
        "        # Apply Extension 2 decoding strategy (adaptive beams + context-aware length penalty)\n",
        "        \"model\": ext3_model,\n",
        "        \"tokenizer\": ext3_tokenizer,\n",
        "        \"fn\": simplify_ext2_adaptive_decoding,\n",
        "    },\n",
        "}\n",
        "\n",
        "all_results: Dict[str, Dict[float, Dict[str, List[float]]]] = {}\n",
        "\n",
        "for name, cfg in SYSTEMS_TO_RUN.items():\n",
        "    all_results[name] = eval_over_completion_ratios(\n",
        "        cfg[\"model\"],\n",
        "        cfg[\"tokenizer\"],\n",
        "        test_df,\n",
        "        cfg[\"fn\"],\n",
        "        label=name,\n",
        "    )\n",
        "\n",
        "# Build a unified summary table (measured for all systems)\n",
        "rows: List[dict] = []\n",
        "\n",
        "for sys_name, res in all_results.items():\n",
        "    for r in COMPLETION_RATIOS:\n",
        "        rows.append(\n",
        "            {\n",
        "                \"system\": sys_name,\n",
        "                \"completion_ratio\": float(r),\n",
        "                \"SARI\": float(np.mean(res[r][\"sari\"])),\n",
        "                \"KEEP\": float(np.mean(res[r][\"keep\"])),\n",
        "                \"DELETE\": float(np.mean(res[r][\"delete\"])),\n",
        "                \"ADD\": float(np.mean(res[r][\"add\"])),\n",
        "                \"latency_ms\": float(np.mean(res[r][\"latency_s\"])) * 1000.0,\n",
        "            }\n",
        "        )\n",
        "\n",
        "summary_df = pd.DataFrame(rows)\n",
        "summary_df[\"completion_pct\"] = (summary_df[\"completion_ratio\"] * 100).astype(int)\n",
        "\n",
        "print(\"Done (evaluated all systems).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea1ce385",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary tables\n",
        "\n",
        "# Keep a stable column ordering for plots/tables\n",
        "SYSTEM_ORDER = [\n",
        "    \"Strong Baseline (fixed)\",\n",
        "    \"Extension 1 (adaptive lenpen)\",\n",
        "    \"Extension 2 (adaptive decoding)\",\n",
        "    \"Extension 3 (arch, fixed)\",\n",
        "    \"Extension 3 + beam-techniques\",\n",
        "]\n",
        "\n",
        "pivot_sari = summary_df.pivot(index=\"completion_pct\", columns=\"system\", values=\"SARI\").reindex(\n",
        "    columns=SYSTEM_ORDER\n",
        ")\n",
        "pivot_lat = summary_df.pivot(index=\"completion_pct\", columns=\"system\", values=\"latency_ms\").reindex(\n",
        "    columns=SYSTEM_ORDER\n",
        ")\n",
        "\n",
        "print(\"SARI (mean):\")\n",
        "display(pivot_sari.round(2))\n",
        "\n",
        "print(\"Latency (ms, mean):\")\n",
        "display(pivot_lat.round(2))\n",
        "\n",
        "overall = (\n",
        "    summary_df.groupby(\"system\")\n",
        "    .agg(\n",
        "        overall_SARI=(\"SARI\", \"mean\"),\n",
        "        overall_latency_ms=(\"latency_ms\", \"mean\"),\n",
        "    )\n",
        "    .reindex(SYSTEM_ORDER)\n",
        ")\n",
        "\n",
        "overall_75 = (\n",
        "    summary_df[summary_df[\"completion_pct\"] != 100].groupby(\"system\")\n",
        "    .agg(\n",
        "        overall_SARI=(\"SARI\", \"mean\"),\n",
        "        overall_latency_ms=(\"latency_ms\", \"mean\"),\n",
        "    )\n",
        "    .reindex(SYSTEM_ORDER)\n",
        ")\n",
        "\n",
        "print(\"Overall averages (mean across completion ratios):\")\n",
        "display(overall.round(3))\n",
        "\n",
        "print(\"Overall averages (mean across completion ratios besides 100%):\")\n",
        "display(overall_75.round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "133eb964",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plots (line + bar) in the same style as extension2.ipynb, now mixing:\n",
        "# - precomputed Baseline/Ext1/Ext2\n",
        "# - measured Ext3 variants\n",
        "\n",
        "ratios = [int(r * 100) for r in COMPLETION_RATIOS]\n",
        "\n",
        "# Build ordered series from pivot tables\n",
        "labels = SYSTEM_ORDER\n",
        "sari_series = {name: pivot_sari[name].tolist() for name in labels}\n",
        "lat_series = {name: pivot_lat[name].tolist() for name in labels}\n",
        "\n",
        "# --- Line plots ---\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for name in labels:\n",
        "    ax1.plot(ratios, sari_series[name], marker=\"o\", linewidth=2, markersize=6, label=name)\n",
        "\n",
        "ax1.set_xlabel(\"Completion Ratio (%)\", fontsize=12)\n",
        "ax1.set_ylabel(\"SARI Score\", fontsize=12)\n",
        "ax1.set_title(\"SARI Scores Across Completion Ratios\", fontsize=14, fontweight=\"bold\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend(fontsize=9)\n",
        "\n",
        "for name in labels:\n",
        "    ax2.plot(ratios, lat_series[name], marker=\"o\", linewidth=2, markersize=6, label=name)\n",
        "\n",
        "ax2.set_xlabel(\"Completion Ratio (%)\", fontsize=12)\n",
        "ax2.set_ylabel(\"Average Latency (ms)\", fontsize=12)\n",
        "ax2.set_title(\"Latency Across Completion Ratios\", fontsize=14, fontweight=\"bold\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_yscale(\"log\")\n",
        "ax2.legend(fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Bar plots ---\n",
        "fig, (bx1, bx2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "x = np.arange(len(ratios))\n",
        "width = 0.16\n",
        "\n",
        "for i, name in enumerate(labels):\n",
        "    offset = (i - (len(labels) - 1) / 2) * width\n",
        "    bx1.bar(x + offset, sari_series[name], width, label=name, alpha=0.85)\n",
        "\n",
        "bx1.set_xlabel(\"Completion Ratio (%)\", fontsize=12)\n",
        "bx1.set_ylabel(\"SARI Score\", fontsize=12)\n",
        "bx1.set_title(\"SARI Scores Comparison (Bar Chart)\", fontsize=14, fontweight=\"bold\")\n",
        "bx1.set_xticks(x)\n",
        "bx1.set_xticklabels(ratios)\n",
        "bx1.grid(True, alpha=0.3, axis=\"y\")\n",
        "bx1.legend(fontsize=9)\n",
        "\n",
        "for i, name in enumerate(labels):\n",
        "    offset = (i - (len(labels) - 1) / 2) * width\n",
        "    bx2.bar(x + offset, lat_series[name], width, label=name, alpha=0.85)\n",
        "\n",
        "bx2.set_xlabel(\"Completion Ratio (%)\", fontsize=12)\n",
        "bx2.set_ylabel(\"Average Latency (ms)\", fontsize=12)\n",
        "bx2.set_title(\"Latency Comparison (Bar Chart)\", fontsize=14, fontweight=\"bold\")\n",
        "bx2.set_xticks(x)\n",
        "bx2.set_xticklabels(ratios)\n",
        "bx2.set_yscale(\"log\")\n",
        "bx2.grid(True, alpha=0.3, axis=\"y\")\n",
        "bx2.legend(fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
