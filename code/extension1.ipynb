{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64XjuioTwCJb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Tuple\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqR2iIVdwneB"
      },
      "outputs": [],
      "source": [
        "def tokenize(text: str) -> List[str]:\n",
        "    return text.lower().split()\n",
        "\n",
        "\n",
        "def get_ngrams(tokens: List[str], n: int) -> Counter:\n",
        "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1))\n",
        "\n",
        "\n",
        "def sari_score(source: str, prediction: str, references: List[str]) -> Tuple[float, dict]:\n",
        "    src_tokens = tokenize(source)\n",
        "    pred_tokens = tokenize(prediction)\n",
        "    ref_tokens_list = [tokenize(ref) for ref in references]\n",
        "\n",
        "    keep_scores = []\n",
        "    del_scores = []\n",
        "    add_scores = []\n",
        "\n",
        "    for n in range(1, 5):\n",
        "        src_ngrams = get_ngrams(src_tokens, n)\n",
        "        pred_ngrams = get_ngrams(pred_tokens, n)\n",
        "        ref_ngrams_list = [get_ngrams(ref_tokens, n) for ref_tokens in ref_tokens_list]\n",
        "\n",
        "        src_in_refs = Counter()\n",
        "        for ngram in src_ngrams:\n",
        "            count = sum(1 for ref_ngrams in ref_ngrams_list if ngram in ref_ngrams)\n",
        "            if count > 0:\n",
        "                src_in_refs[ngram] = count\n",
        "\n",
        "        kept_ngrams = Counter()\n",
        "        for ngram in pred_ngrams:\n",
        "            if ngram in src_ngrams:\n",
        "                kept_ngrams[ngram] = min(pred_ngrams[ngram], src_ngrams[ngram])\n",
        "\n",
        "        keep_prec_num = sum(min(kept_ngrams[ng], 1) for ng in kept_ngrams if ng in src_in_refs)\n",
        "        keep_prec_den = sum(kept_ngrams.values())\n",
        "        keep_prec = keep_prec_num / keep_prec_den if keep_prec_den > 0 else 0\n",
        "\n",
        "        keep_rec_num = sum(min(kept_ngrams[ng], 1) for ng in src_in_refs if ng in kept_ngrams)\n",
        "        keep_rec_den = len(src_in_refs)\n",
        "        keep_rec = keep_rec_num / keep_rec_den if keep_rec_den > 0 else 0\n",
        "\n",
        "        keep_f1 = 2 * keep_prec * keep_rec / (keep_prec + keep_rec) if (keep_prec + keep_rec) > 0 else 0\n",
        "        keep_scores.append(keep_f1)\n",
        "\n",
        "        src_not_in_refs = set(ng for ng in src_ngrams if ng not in src_in_refs)\n",
        "        deleted_ngrams = set(ng for ng in src_ngrams if ng not in pred_ngrams)\n",
        "\n",
        "        del_prec_num = len(deleted_ngrams & src_not_in_refs)\n",
        "        del_prec_den = len(deleted_ngrams)\n",
        "        del_prec = del_prec_num / del_prec_den if del_prec_den > 0 else 0\n",
        "\n",
        "        del_rec_num = len(deleted_ngrams & src_not_in_refs)\n",
        "        del_rec_den = len(src_not_in_refs)\n",
        "        del_rec = del_rec_num / del_rec_den if del_rec_den > 0 else 0\n",
        "\n",
        "        del_f1 = 2 * del_prec * del_rec / (del_prec + del_rec) if (del_prec + del_rec) > 0 else 0\n",
        "        del_scores.append(del_f1)\n",
        "\n",
        "        added_ngrams = set(ng for ng in pred_ngrams if ng not in src_ngrams)\n",
        "\n",
        "        ref_not_in_src = set()\n",
        "        for ref_ngrams in ref_ngrams_list:\n",
        "            ref_not_in_src.update(ng for ng in ref_ngrams if ng not in src_ngrams)\n",
        "\n",
        "        add_prec_num = len(added_ngrams & ref_not_in_src)\n",
        "        add_prec_den = len(added_ngrams)\n",
        "        add_prec = add_prec_num / add_prec_den if add_prec_den > 0 else 0\n",
        "\n",
        "        add_scores.append(add_prec)\n",
        "\n",
        "    keep_avg = sum(keep_scores) / 4\n",
        "    del_avg = sum(del_scores) / 4\n",
        "    add_avg = sum(add_scores) / 4\n",
        "\n",
        "    sari = (keep_avg + del_avg + add_avg) / 3 * 100\n",
        "\n",
        "    components = {\n",
        "        'keep': keep_avg * 100,\n",
        "        'delete': del_avg * 100,\n",
        "        'add': add_avg * 100\n",
        "    }\n",
        "\n",
        "    return sari, components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0EMqk6JwLq_"
      },
      "outputs": [],
      "source": [
        "class SimplificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, sources: List[str], targets: List[str], tokenizer, max_length: int = 128):\n",
        "        self.sources = sources\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sources)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source = \"simplify: \" + self.sources[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        source_enc = self.tokenizer(\n",
        "            source,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        target_enc = self.tokenizer(\n",
        "            target,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        labels = target_enc['input_ids'].squeeze()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': source_enc['input_ids'].squeeze(),\n",
        "            'attention_mask': source_enc['attention_mask'].squeeze(),\n",
        "            'labels': labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7YhfMDJwOpP"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_sources: List[str],\n",
        "    train_targets: List[str],\n",
        "    val_sources: List[str],\n",
        "    val_targets: List[str],\n",
        "    output_dir: str,\n",
        "    epochs: int = 3,\n",
        "    batch_size: int = 16,\n",
        "    learning_rate: float = 3e-5,\n",
        "    device: str = 'cuda'\n",
        "):\n",
        "    train_dataset = SimplificationDataset(train_sources, train_targets, tokenizer)\n",
        "    val_dataset = SimplificationDataset(val_sources, val_targets, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                val_loss += outputs.loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            model.save_pretrained(output_dir)\n",
        "            tokenizer.save_pretrained(output_dir)\n",
        "            print(f\"Saved best model to {output_dir}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM2okXDlwWUq"
      },
      "outputs": [],
      "source": [
        "def strong_baseline(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    max_length: int = 128,\n",
        "    num_beams: int = 4,\n",
        "    device: str = 'cuda'\n",
        ") -> str:\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_text = \"simplify: \" + source\n",
        "        input_enc = tokenizer(\n",
        "            input_text,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_enc['input_ids'],\n",
        "            attention_mask=input_enc['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6nvsoCwwXGV",
        "outputId": "911d7348-8d41-4a95-c77d-35347f93595a"
      },
      "outputs": [],
      "source": [
        "model_name = 't5-small'\n",
        "output_dir = './t5-simplification'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 20\n",
        "batch_size = 16\n",
        "learning_rate = 3e-5\n",
        "\n",
        "splits = {\n",
        "    'train': 'wiki.full.aner.ori.train.95.tsv',\n",
        "    'validation': 'wiki.full.aner.ori.valid.95.tsv',\n",
        "    'test': 'wiki.full.aner.ori.test.95.tsv'\n",
        "}\n",
        "\n",
        "print(\"Loading WikiLarge dataset...\")\n",
        "train = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"train\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "val = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"validation\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "test = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"test\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "model = train_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_sources=train['Normal'].tolist(),\n",
        "    train_targets=train['Simple'].tolist(),\n",
        "    val_sources=val['Normal'].tolist(),\n",
        "    val_targets=val['Simple'].tolist(),\n",
        "    output_dir=output_dir,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "outputs = []\n",
        "for source in tqdm(test['Normal'], desc=\"Generating simplifications\"):\n",
        "    output = strong_baseline(model, tokenizer, source, device=device)\n",
        "    outputs.append(output)\n",
        "\n",
        "\n",
        "sari_scores = []\n",
        "keep_scores = []\n",
        "del_scores = []\n",
        "add_scores = []\n",
        "\n",
        "for output, source, reference in zip(outputs, test['Normal'], test['Simple']):\n",
        "    sari, components = sari_score(source, output, [reference])\n",
        "    sari_scores.append(sari)\n",
        "    keep_scores.append(components['keep'])\n",
        "    del_scores.append(components['delete'])\n",
        "    add_scores.append(components['add'])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Strong Baseline SARI Score Results\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Number of samples: {len(sari_scores)}\")\n",
        "print()\n",
        "print(f\"  SARI:        {sum(sari_scores) / len(sari_scores):.2f}\")\n",
        "print(f\"    - Keep:    {sum(keep_scores) / len(keep_scores):.2f}\")\n",
        "print(f\"    - Delete:  {sum(del_scores) / len(del_scores):.2f}\")\n",
        "print(f\"    - Add:     {sum(add_scores) / len(add_scores):.2f}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXo8Mp_fo8yZ",
        "outputId": "c728e9bd-70f4-4e2d-ec89-3bf5b176a5b8"
      },
      "outputs": [],
      "source": [
        "!zip -r t5-simplification.zip /content/t5-simplification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KLfzLGIgpBN3",
        "outputId": "98ee885a-2256-41ff-969d-8cf4babbeb12"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('t5-simplification.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aec905c"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('t5-simplification.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKZrQw59ozY-",
        "outputId": "fd0517de-6c72-40df-b718-b22996effdfb"
      },
      "outputs": [],
      "source": [
        "# output_dir = 'content/t5-simplification'\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = T5ForConditionalGeneration.from_pretrained(output_dir)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "# model.to(device)\n",
        "\n",
        "# test = pd.read_csv(\n",
        "#     \"hf://datasets/bogdancazan/wikilarge-text-simplification/wiki.full.aner.ori.test.95.tsv\",\n",
        "#     sep=\"\\t\"\n",
        "# )\n",
        "\n",
        "# print(\"Generating predictions and calculating scores...\")\n",
        "# results = []\n",
        "# for idx, row in tqdm(test.iterrows(), total=len(test)):\n",
        "#     source = row['Normal']\n",
        "#     reference = row['Simple']\n",
        "\n",
        "#     output = strong_baseline(model, tokenizer, source, device=device)\n",
        "\n",
        "#     sari, components = sari_score(source, output, [reference])\n",
        "\n",
        "#     results.append({\n",
        "#         'idx': idx,\n",
        "#         'source': source,\n",
        "#         'reference': reference,\n",
        "#         'prediction': output,\n",
        "#         'sari': sari,\n",
        "#         'keep': components['keep'],\n",
        "#         'delete': components['delete'],\n",
        "#         'add': components['add']\n",
        "#     })\n",
        "\n",
        "# results_df = pd.DataFrame(results)\n",
        "\n",
        "# results_df = results_df.sort_values('sari', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*80)\n",
        "# print(\"TOP 5 EXAMPLES (Highest SARI Scores)\")\n",
        "# print(\"=\"*80)\n",
        "# for i, row in results_df.head(5).iterrows():\n",
        "#     print(f\"\\nExample {i+1} - SARI: {row['sari']:.2f} (Keep: {row['keep']:.2f}, Del: {row['delete']:.2f}, Add: {row['add']:.2f})\")\n",
        "#     print(f\"Source:     {row['source']}\")\n",
        "#     print(f\"Reference:  {row['reference']}\")\n",
        "#     print(f\"Prediction: {row['prediction']}\")\n",
        "#     print(\"-\"*80)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*80)\n",
        "# print(\"BOTTOM 5 EXAMPLES (Lowest SARI Scores)\")\n",
        "# print(\"=\"*80)\n",
        "# for i, row in results_df.tail(5).iterrows():\n",
        "#     print(f\"\\nExample {i+1} - SARI: {row['sari']:.2f} (Keep: {row['keep']:.2f}, Del: {row['delete']:.2f}, Add: {row['add']:.2f})\")\n",
        "#     print(f\"Source:     {row['source']}\")\n",
        "#     print(f\"Reference:  {row['reference']}\")\n",
        "#     print(f\"Prediction: {row['prediction']}\")\n",
        "#     print(\"-\"*80)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*80)\n",
        "# print(\"SUMMARY STATISTICS\")\n",
        "# print(\"=\"*80)\n",
        "# print(f\"Total examples: {len(results_df)}\")\n",
        "# print(f\"\\nSARI Score Distribution:\")\n",
        "# print(f\"  Mean:   {results_df['sari'].mean():.2f}\")\n",
        "# print(f\"  Median: {results_df['sari'].median():.2f}\")\n",
        "# print(f\"  Std:    {results_df['sari'].std():.2f}\")\n",
        "# print(f\"  Min:    {results_df['sari'].min():.2f}\")\n",
        "# print(f\"  Max:    {results_df['sari'].max():.2f}\")\n",
        "# print(\"=\"*80)\n",
        "\n",
        "# results_df.to_csv('model_analysis_results.csv', index=False)\n",
        "# print(f\"\\nResults saved to 'model_analysis_results.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdu1bSc33rN5"
      },
      "outputs": [],
      "source": [
        "def get_sentence_prefix(source: str, completion_ratio: float) -> str:\n",
        "    tokens = source.split()\n",
        "    num_tokens = len(tokens)\n",
        "    prefix_length = max(1, int(num_tokens * completion_ratio))\n",
        "\n",
        "    prefix_tokens = tokens[:prefix_length]\n",
        "    return ' '.join(prefix_tokens)\n",
        "\n",
        "\n",
        "def incremental_simplify(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    completion_ratio: float,\n",
        "    max_length: int = 128,\n",
        "    num_beams: int = 4,\n",
        "    length_penalty: float = 0.6,\n",
        "    device: str = 'cuda'\n",
        ") -> str:\n",
        "    prefix = get_sentence_prefix(source, completion_ratio)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_text = \"simplify: \" + prefix\n",
        "        input_enc = tokenizer(\n",
        "            input_text,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_enc['input_ids'],\n",
        "            attention_mask=input_enc['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=length_penalty,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "\n",
        "def adaptive_incremental_simplify(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    completion_ratio: float,\n",
        "    max_length: int = 128,\n",
        "    num_beams: int = 4,\n",
        "    device: str = 'cuda'\n",
        ") -> str:\n",
        "    if completion_ratio <= 0.25:\n",
        "        length_penalty = 0.4\n",
        "    elif completion_ratio <= 0.5:\n",
        "        length_penalty = 0.5\n",
        "    elif completion_ratio <= 0.75:\n",
        "        length_penalty = 0.6\n",
        "    else:\n",
        "        length_penalty = 0.7\n",
        "\n",
        "    return incremental_simplify(\n",
        "        model, tokenizer, source, completion_ratio,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=length_penalty,\n",
        "        device=device\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GToebvC_4Ra-",
        "outputId": "03ef78ef-d808-47bf-fcd5-760930c491c3"
      },
      "outputs": [],
      "source": [
        "model_dir = 'content/t5-simplification'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_length = 128\n",
        "\n",
        "model_dir = os.path.abspath(model_dir)\n",
        "\n",
        "splits = {\n",
        "    'test': 'wiki.full.aner.ori.test.95.tsv'\n",
        "}\n",
        "\n",
        "print(\"Loading fine-tuned T5 model...\")\n",
        "if not os.path.exists(model_dir):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Model directory {model_dir} not found. \"\n",
        "        \"Please run strong_baseline.py first to train the model.\"\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir, local_files_only=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "print(\"Loading WikiLarge test dataset...\")\n",
        "test = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"test\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "print(f\"Loaded {len(test)} test examples\")\n",
        "\n",
        "completion_ratios = [0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "print(\"\\nEvaluating incremental simplification with fixed length penalty...\")\n",
        "fixed_results = {ratio: {'sari': [], 'keep': [], 'delete': [], 'add': []} for ratio in completion_ratios}\n",
        "\n",
        "for idx, row in tqdm(test.iterrows(), total=len(test), desc=\"Processing\"):\n",
        "    source = row['Normal']\n",
        "    reference = row['Simple']\n",
        "\n",
        "    for ratio in completion_ratios:\n",
        "        output = incremental_simplify(\n",
        "            model, tokenizer, source, ratio,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            length_penalty=0.6,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        sari, components = sari_score(source, output, [reference])\n",
        "        fixed_results[ratio]['sari'].append(sari)\n",
        "        fixed_results[ratio]['keep'].append(components['keep'])\n",
        "        fixed_results[ratio]['delete'].append(components['delete'])\n",
        "        fixed_results[ratio]['add'].append(components['add'])\n",
        "\n",
        "print(\"\\nEvaluating incremental simplification with adaptive length penalty...\")\n",
        "adaptive_results = {ratio: {'sari': [], 'keep': [], 'delete': [], 'add': []} for ratio in completion_ratios}\n",
        "\n",
        "for idx, row in tqdm(test.iterrows(), total=len(test), desc=\"Processing\"):\n",
        "    source = row['Normal']\n",
        "    reference = row['Simple']\n",
        "\n",
        "    for ratio in completion_ratios:\n",
        "        output = adaptive_incremental_simplify(\n",
        "            model, tokenizer, source, ratio,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        sari, components = sari_score(source, output, [reference])\n",
        "        adaptive_results[ratio]['sari'].append(sari)\n",
        "        adaptive_results[ratio]['keep'].append(components['keep'])\n",
        "        adaptive_results[ratio]['delete'].append(components['delete'])\n",
        "        adaptive_results[ratio]['add'].append(components['add'])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Fixed Length Penalty Results\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n{'Completion Ratio':<20} {'SARI':<10} {'Keep':<10} {'Delete':<10} {'Add':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    sari_avg = np.mean(fixed_results[ratio]['sari'])\n",
        "    keep_avg = np.mean(fixed_results[ratio]['keep'])\n",
        "    del_avg = np.mean(fixed_results[ratio]['delete'])\n",
        "    add_avg = np.mean(fixed_results[ratio]['add'])\n",
        "\n",
        "    print(f\"{ratio*100:>5.0f}%{'':<14} {sari_avg:>6.2f}    {keep_avg:>6.2f}    {del_avg:>6.2f}    {add_avg:>6.2f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Adaptive Length Penalty Results\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n{'Completion Ratio':<20} {'SARI':<10} {'Keep':<10} {'Delete':<10} {'Add':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    sari_avg = np.mean(adaptive_results[ratio]['sari'])\n",
        "    keep_avg = np.mean(adaptive_results[ratio]['keep'])\n",
        "    del_avg = np.mean(adaptive_results[ratio]['delete'])\n",
        "    add_avg = np.mean(adaptive_results[ratio]['add'])\n",
        "\n",
        "    print(f\"{ratio*100:>5.0f}%{'':<14} {sari_avg:>6.2f}    {keep_avg:>6.2f}    {del_avg:>6.2f}    {add_avg:>6.2f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Comparison: Fixed vs Adaptive Length Penalty\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n{'Completion Ratio':<20} {'Fixed SARI':<15} {'Adaptive SARI':<15} {'Improvement':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    fixed_sari = np.mean(fixed_results[ratio]['sari'])\n",
        "    adaptive_sari = np.mean(adaptive_results[ratio]['sari'])\n",
        "    improvement = adaptive_sari - fixed_sari\n",
        "\n",
        "    print(f\"{ratio*100:>5.0f}%{'':<14} {fixed_sari:>8.2f}      {adaptive_sari:>8.2f}      {improvement:>+8.2f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "full_context_sari = np.mean(adaptive_results[1.0]['sari'])\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Quality Degradation Analysis (Adaptive Strategy)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nFull context (100%) SARI: {full_context_sari:.2f}\")\n",
        "print(f\"\\n{'Completion Ratio':<20} {'SARI':<15} {'% of Full Quality':<20}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    sari = np.mean(adaptive_results[ratio]['sari'])\n",
        "    pct_quality = (sari / full_context_sari) * 100 if full_context_sari > 0 else 0\n",
        "    print(f\"{ratio*100:>5.0f}%{'':<14} {sari:>8.2f}      {pct_quality:>15.1f}%\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Example: Incremental Simplification\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "example_idx = 0\n",
        "source = test.iloc[example_idx]['Normal']\n",
        "reference = test.iloc[example_idx]['Simple']\n",
        "\n",
        "print(f\"\\nSource: {source}\")\n",
        "print(f\"Reference: {reference}\")\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    prefix = get_sentence_prefix(source, ratio)\n",
        "    output = adaptive_incremental_simplify(\n",
        "        model, tokenizer, source, ratio,\n",
        "        max_length=max_length,\n",
        "        num_beams=4,\n",
        "        device=device\n",
        "    )\n",
        "    sari, _ = sari_score(source, output, [reference])\n",
        "\n",
        "    print(f\"\\n[{ratio*100:.0f}% context]\")\n",
        "    print(f\"  Prefix: {prefix}\")\n",
        "    print(f\"  Output: {output}\")\n",
        "    print(f\"  SARI: {sari:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
