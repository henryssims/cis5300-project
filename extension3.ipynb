{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c50bfc06",
      "metadata": {},
      "source": [
        "## Extension 3: Train `t5-simplification-2` (T5 + Encoder Adapter)\n",
        "\n",
        "This notebook trains a *new* model directory `t5-simplification-2/` end-to-end on this project’s `data/train.csv` and validates on `data/dev.csv`.\n",
        "\n",
        "### Architectural change (vs. prior T5 fine-tuning)\n",
        "Instead of using plain `T5ForConditionalGeneration`, we add a **learned bottleneck adapter** applied to the encoder hidden states *before* decoding:\n",
        "\n",
        "- Compute encoder hidden states `H` with T5 encoder.\n",
        "- Apply a residual adapter: `H' = LN(H + gate * Up(GELU(Down(H))))`.\n",
        "\n",
        "Intuition: simplification often needs a compact “rewrite transform” between input semantics and surface form. The adapter adds capacity targeted at this transformation without changing the tokenizer or decoding interface.\n",
        "\n",
        "### Relation to real-time simplification\n",
        "Since extension 2 was able to significantly reduce latency in real-time settings, this change is more meant to improve **quality-per-compute** in real-time settings (better outputs at the same decoding settings), not to dramatically reduce latency.\n",
        "\n",
        "- **Why it can help**: the adapter adds small, targeted capacity for rewriting while keeping the same tokenizer + generation interface.\n",
        "- **Latency impact**: it adds one extra MLP pass over encoder hidden states (usually minor). Most inference time is still dominated by autoregressive decoding (number of generated tokens × `num_beams`).\n",
        "- **If you need strict lower latency**: the biggest knobs are typically lowering `num_beams`, lowering `max_length`, and/or using distillation/quantization; this adapter is complementary because it can help preserve quality when you dial beams down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29c5ea57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies if missing (safe to re-run)\n",
        "import importlib\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def _ensure(pkg, pip_name=None):\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pip_name or pkg])\n",
        "\n",
        "\n",
        "_ensure('numpy')\n",
        "_ensure('pandas')\n",
        "_ensure('tqdm')\n",
        "_ensure('torch')\n",
        "_ensure('transformers')\n",
        "_ensure('sentencepiece', 'sentencepiece')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784351ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "from score import sari_score\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a0e31d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config\n",
        "model_name = 't5-small'\n",
        "output_dir = './t5-simplification-2'\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "num_beams = 6\n",
        "\n",
        "# Training schedule\n",
        "adapter_bottleneck = 256\n",
        "adapter_dropout = 0.1\n",
        "\n",
        "# Match the stronger baseline-style training budget (~20 epochs total):\n",
        "# short warmup where only the adapter learns, then full fine-tuning.\n",
        "phase1_epochs = 1      # train adapter only\n",
        "phase2_epochs = 1     # fine-tune full model + adapter\n",
        "\n",
        "lr_adapter_phase1 = 1e-3\n",
        "lr_full_phase2 = 3e-5\n",
        "grad_clip = 1.0\n",
        "\n",
        "# Fast validation during training (full dev eval happens later)\n",
        "quick_dev_eval_n = 200\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60649899",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load project CSVs\n",
        "train_df = pd.read_csv('data/train.csv')\n",
        "dev_df = pd.read_csv('data/dev.csv')\n",
        "test_df = pd.read_csv('data/test.csv')\n",
        "\n",
        "for df_name, df in [('train', train_df), ('dev', dev_df), ('test', test_df)]:\n",
        "    if not {'Normal', 'Simple'}.issubset(df.columns):\n",
        "        raise ValueError(f'{df_name}.csv must have Normal,Simple columns; got {df.columns.tolist()}')\n",
        "    df['Normal'] = df['Normal'].astype(str)\n",
        "    df['Simple'] = df['Simple'].astype(str)\n",
        "\n",
        "print('train/dev/test sizes:', len(train_df), len(dev_df), len(test_df))\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e8ed584",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimplificationDataset(Dataset):\n",
        "    def __init__(self, sources, targets, tokenizer, max_length: int = 128):\n",
        "        self.sources = list(sources)\n",
        "        self.targets = list(targets)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sources)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source = 'simplify: ' + self.sources[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        source_enc = self.tokenizer(\n",
        "            source,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        target_enc = self.tokenizer(\n",
        "            target,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        labels = target_enc['input_ids'].squeeze(0)\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': source_enc['input_ids'].squeeze(0),\n",
        "            'attention_mask': source_enc['attention_mask'].squeeze(0),\n",
        "            'labels': labels,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c04895ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBottleneckAdapter(nn.Module):\n",
        "    def __init__(self, d_model: int, bottleneck: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.down = nn.Linear(d_model, bottleneck)\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.up = nn.Linear(bottleneck, d_model)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        # Trainable scalar gate initialized near 0: start close to plain T5\n",
        "        self.gate = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, h: torch.Tensor) -> torch.Tensor:\n",
        "        delta = self.up(self.dropout(self.act(self.down(h))))\n",
        "        return self.ln(h + torch.tanh(self.gate) * delta)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AdapterConfig:\n",
        "    bottleneck: int\n",
        "    dropout: float\n",
        "\n",
        "\n",
        "class T5WithEncoderAdapter(nn.Module):\n",
        "    def __init__(self, base_model_name: str, bottleneck: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.base = T5ForConditionalGeneration.from_pretrained(base_model_name)\n",
        "        d_model = self.base.config.d_model\n",
        "        self.adapter = EncoderBottleneckAdapter(d_model=d_model, bottleneck=bottleneck, dropout=dropout)\n",
        "        self.adapter_cfg = AdapterConfig(bottleneck=bottleneck, dropout=dropout)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
        "        # Run encoder once, then adapt encoder hidden states before decoding.\n",
        "        enc = self.base.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        h = self.adapter(enc.last_hidden_state)\n",
        "        encoder_outputs = BaseModelOutput(\n",
        "            last_hidden_state=h,\n",
        "            hidden_states=enc.hidden_states,\n",
        "            attentions=enc.attentions,\n",
        "        )\n",
        "\n",
        "        return self.base(\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            return_dict=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids=None, attention_mask=None, **gen_kwargs):\n",
        "        enc = self.base.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        h = self.adapter(enc.last_hidden_state)\n",
        "        encoder_outputs = BaseModelOutput(\n",
        "            last_hidden_state=h,\n",
        "            hidden_states=enc.hidden_states,\n",
        "            attentions=enc.attentions,\n",
        "        )\n",
        "        return self.base.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            **gen_kwargs,\n",
        "        )\n",
        "\n",
        "    def save(self, output_dir: str, tokenizer):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        base_dir = os.path.join(output_dir, 'base')\n",
        "        self.base.save_pretrained(base_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        torch.save(self.adapter.state_dict(), os.path.join(output_dir, 'adapter.pt'))\n",
        "        with open(os.path.join(output_dir, 'adapter_config.json'), 'w') as f:\n",
        "            json.dump({\n",
        "                'bottleneck': self.adapter_cfg.bottleneck,\n",
        "                'dropout': self.adapter_cfg.dropout,\n",
        "            }, f, indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, output_dir: str):\n",
        "        base_dir = os.path.join(output_dir, 'base')\n",
        "        with open(os.path.join(output_dir, 'adapter_config.json'), 'r') as f:\n",
        "            cfg = json.load(f)\n",
        "        model = cls(base_model_name=base_dir, bottleneck=cfg['bottleneck'], dropout=cfg['dropout'])\n",
        "        sd = torch.load(os.path.join(output_dir, 'adapter.pt'), map_location='cpu')\n",
        "        model.adapter.load_state_dict(sd)\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcf5be7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_base_trainable(model: T5WithEncoderAdapter, trainable: bool):\n",
        "    for p in model.base.parameters():\n",
        "        p.requires_grad = trainable\n",
        "    for p in model.adapter.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "\n",
        "def train_one_epoch(model, train_loader, optimizer, scheduler=None):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for batch in tqdm(train_loader, desc='train', leave=False):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        out = model(**batch)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        total += loss.item()\n",
        "    return total / max(1, len(train_loader))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loss(model, val_loader):\n",
        "    model.eval()\n",
        "    total = 0.0\n",
        "    for batch in tqdm(val_loader, desc='val', leave=False):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        out = model(**batch)\n",
        "        total += out.loss.item()\n",
        "    return total / max(1, len(val_loader))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text(model, tokenizer, sources):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for src in tqdm(list(sources), desc='generate', leave=False):\n",
        "        enc = tokenizer(\n",
        "            'simplify: ' + str(src),\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt',\n",
        "        ).to(device)\n",
        "        out_ids = model.generate(\n",
        "            input_ids=enc['input_ids'],\n",
        "            attention_mask=enc['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3,\n",
        "        )\n",
        "        preds.append(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n",
        "    return preds\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_sari(model, tokenizer, df, n: int = 200):\n",
        "    if n is None or n >= len(df):\n",
        "        small = df\n",
        "    else:\n",
        "        small = df.sample(n=n, random_state=5300).reset_index(drop=True)\n",
        "\n",
        "    preds = generate_text(model, tokenizer, small['Normal'])\n",
        "\n",
        "    sari_scores = []\n",
        "    for pred, src, ref in zip(preds, small['Normal'], small['Simple']):\n",
        "        s, _ = sari_score(str(src), str(pred), [str(ref)])\n",
        "        sari_scores.append(s)\n",
        "    return float(np.mean(sari_scores))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e66131d",
      "metadata": {},
      "source": [
        "### (Optional / Experimental) Limited attention buffer for real-time decoding\n",
        "\n",
        "If you want to simulate a *limited attention buffer* during generation, a practical option for T5 is to cap the **decoder self-attention history** to the last *N* generated tokens (a sliding KV-cache window).\n",
        "\n",
        "- This can reduce **memory** and sometimes **latency** when outputs get long.\n",
        "- For sentence simplification, outputs are typically short, so the biggest real-time levers are still `num_beams`, `max_length`/`max_new_tokens`, and batching.\n",
        "- This implementation uses **greedy decoding** (beam search + cache-windowing is more complex)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8905692",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def _truncate_t5_past_key_values(past_key_values, window: int):\n",
        "    \"\"\"Keep only the last `window` tokens of decoder self-attention KV.\n",
        "\n",
        "    T5 past_key_values is a list/tuple over layers.\n",
        "    Each layer typically has: (self_k, self_v, cross_k, cross_v).\n",
        "    We truncate only self_k/self_v on the sequence-length dimension.\n",
        "    \"\"\"\n",
        "    if past_key_values is None or window is None:\n",
        "        return past_key_values\n",
        "\n",
        "    new_past = []\n",
        "    for layer_past in past_key_values:\n",
        "        if layer_past is None:\n",
        "            new_past.append(layer_past)\n",
        "            continue\n",
        "\n",
        "        # Expected: (self_k, self_v, cross_k, cross_v)\n",
        "        if len(layer_past) < 2:\n",
        "            new_past.append(layer_past)\n",
        "            continue\n",
        "\n",
        "        self_k, self_v = layer_past[0], layer_past[1]\n",
        "        rest = tuple(layer_past[2:])\n",
        "\n",
        "        # Shapes are usually (batch, heads, seq_len, head_dim)\n",
        "        if self_k is not None and self_k.ndim >= 3:\n",
        "            self_k = self_k[:, :, -window:, :]\n",
        "        if self_v is not None and self_v.ndim >= 3:\n",
        "            self_v = self_v[:, :, -window:, :]\n",
        "\n",
        "        new_past.append((self_k, self_v) + rest)\n",
        "\n",
        "    return tuple(new_past)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text_greedy_limited_buffer(model: T5WithEncoderAdapter, tokenizer, sources, *,\n",
        "                                       max_new_tokens: int = 128,\n",
        "                                       decoder_buffer_tokens: int | None = 64):\n",
        "    \"\"\"Greedy decoding with an optional sliding decoder attention buffer.\"\"\"\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    for src in tqdm(list(sources), desc='generate(greedy+buffer)', leave=False):\n",
        "        enc = tokenizer(\n",
        "            'simplify: ' + str(src),\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt',\n",
        "        ).to(device)\n",
        "\n",
        "        # Compute adapted encoder outputs once\n",
        "        enc_out = model.base.encoder(\n",
        "            input_ids=enc['input_ids'],\n",
        "            attention_mask=enc['attention_mask'],\n",
        "            return_dict=True,\n",
        "        )\n",
        "        adapted = model.adapter(enc_out.last_hidden_state)\n",
        "        encoder_outputs = BaseModelOutput(last_hidden_state=adapted)\n",
        "\n",
        "        # T5 uses pad_token_id as decoder start token\n",
        "        decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]], device=device)\n",
        "        past = None\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Keep decoder_input_ids short too (optional; main savings is truncating `past`)\n",
        "            if decoder_buffer_tokens is not None:\n",
        "                dec_ids = decoder_input_ids[:, -decoder_buffer_tokens:]\n",
        "                past = _truncate_t5_past_key_values(past, decoder_buffer_tokens)\n",
        "            else:\n",
        "                dec_ids = decoder_input_ids\n",
        "\n",
        "            out = model.base(\n",
        "                encoder_outputs=encoder_outputs,\n",
        "                attention_mask=enc['attention_mask'],\n",
        "                decoder_input_ids=dec_ids,\n",
        "                use_cache=True,\n",
        "                past_key_values=past,\n",
        "                return_dict=True,\n",
        "            )\n",
        "\n",
        "            next_token = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "            decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=1)\n",
        "            past = out.past_key_values\n",
        "\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        preds.append(tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True))\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "# Example usage (optional):\n",
        "# dev_preds_fast = generate_text_greedy_limited_buffer(best_model, best_tokenizer, dev_df['Normal'], max_new_tokens=64, decoder_buffer_tokens=64)\n",
        "# print(dev_preds_fast[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5WithEncoderAdapter(model_name, bottleneck=adapter_bottleneck, dropout=adapter_dropout)\n",
        "model.to(device)\n",
        "\n",
        "train_ds = SimplificationDataset(train_df['Normal'], train_df['Simple'], tokenizer, max_length=max_length)\n",
        "dev_ds = SimplificationDataset(dev_df['Normal'], dev_df['Simple'], tokenizer, max_length=max_length)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "best_quick_sari = -1.0\n",
        "\n",
        "# Phase 1: train adapter only\n",
        "set_base_trainable(model, trainable=False)\n",
        "opt = AdamW(model.adapter.parameters(), lr=lr_adapter_phase1)\n",
        "total_steps = len(train_loader) * phase1_epochs\n",
        "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=max(1, int(0.1 * total_steps)), num_training_steps=total_steps)\n",
        "\n",
        "for epoch in range(phase1_epochs):\n",
        "    tr_loss = train_one_epoch(model, train_loader, opt, sched)\n",
        "    va_loss = eval_loss(model, dev_loader)\n",
        "    quick_sari = eval_sari(model, tokenizer, dev_df, n=quick_dev_eval_n)\n",
        "    print(f'[phase1 epoch {epoch+1}/{phase1_epochs}] train_loss={tr_loss:.4f} dev_loss={va_loss:.4f} quick_dev_SARI={quick_sari:.2f}')\n",
        "    if quick_sari > best_quick_sari:\n",
        "        best_quick_sari = quick_sari\n",
        "        model.save(output_dir, tokenizer)\n",
        "        print(f'  saved best so far -> {output_dir} (quick_dev_SARI={best_quick_sari:.2f})')\n",
        "\n",
        "# Phase 2: fine-tune full model + adapter\n",
        "set_base_trainable(model, trainable=True)\n",
        "opt = AdamW(model.parameters(), lr=lr_full_phase2)\n",
        "total_steps = len(train_loader) * phase2_epochs\n",
        "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=max(1, int(0.1 * total_steps)), num_training_steps=total_steps)\n",
        "\n",
        "for epoch in range(phase2_epochs):\n",
        "    tr_loss = train_one_epoch(model, train_loader, opt, sched)\n",
        "    va_loss = eval_loss(model, dev_loader)\n",
        "    quick_sari = eval_sari(model, tokenizer, dev_df, n=quick_dev_eval_n)\n",
        "    print(f'[phase2 epoch {epoch+1}/{phase2_epochs}] train_loss={tr_loss:.4f} dev_loss={va_loss:.4f} quick_dev_SARI={quick_sari:.2f}')\n",
        "    if quick_sari > best_quick_sari:\n",
        "        best_quick_sari = quick_sari\n",
        "        model.save(output_dir, tokenizer)\n",
        "        print(f'  saved best so far -> {output_dir} (quick_dev_SARI={best_quick_sari:.2f})')\n",
        "\n",
        "print('best quick dev SARI:', best_quick_sari)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload best saved model + tokenizer, then run full dev evaluation\n",
        "best_model = T5WithEncoderAdapter.load(output_dir).to(device)\n",
        "best_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "dev_preds = generate_text(best_model, best_tokenizer, dev_df['Normal'])\n",
        "\n",
        "sari_scores, keep_scores, del_scores, add_scores = [], [], [], []\n",
        "for pred, src, ref in zip(dev_preds, dev_df['Normal'], dev_df['Simple']):\n",
        "    sari, comp = sari_score(str(src), str(pred), [str(ref)])\n",
        "    sari_scores.append(sari)\n",
        "    keep_scores.append(comp['keep'])\n",
        "    del_scores.append(comp['delete'])\n",
        "    add_scores.append(comp['add'])\n",
        "\n",
        "print('='*50)\n",
        "print('Extension3 (T5+Adapter) DEV SARI Score Results')\n",
        "print('='*50)\n",
        "print(f'Number of samples: {len(sari_scores)}')\n",
        "print()\n",
        "print(f'  SARI:        {np.mean(sari_scores):.2f}')\n",
        "print(f'    - Keep:    {np.mean(keep_scores):.2f}')\n",
        "print(f'    - Delete:  {np.mean(del_scores):.2f}')\n",
        "print(f'    - Add:     {np.mean(add_scores):.2f}')\n",
        "print('='*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c83d24c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Results add-on: limited decoder self-attention context (sliding window) ---\n",
        "# This compares the existing “normal” generation (dev_preds, via beam search) to greedy decoding\n",
        "# with/without a limited decoder self-attention buffer.\n",
        "\n",
        "import time\n",
        "import importlib\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def _ensure_local(pkg, pip_name=None):\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pip_name or pkg])\n",
        "\n",
        "\n",
        "_ensure_local('matplotlib', 'matplotlib')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _truncate_t5_self_kv_past(past_key_values, window):\n",
        "    \"\"\"Truncate only decoder self-attn KV history to last `window` tokens.\"\"\"\n",
        "    if past_key_values is None or window is None:\n",
        "        return past_key_values\n",
        "\n",
        "    new_past = []\n",
        "    for layer_past in past_key_values:\n",
        "        # Expected T5 format per-layer: (self_k, self_v, cross_k, cross_v)\n",
        "        if layer_past is None:\n",
        "            new_past.append(layer_past)\n",
        "            continue\n",
        "\n",
        "        if len(layer_past) < 2:\n",
        "            new_past.append(layer_past)\n",
        "            continue\n",
        "\n",
        "        self_k, self_v = layer_past[0], layer_past[1]\n",
        "        rest = tuple(layer_past[2:])\n",
        "\n",
        "        # Typical shape: (batch, heads, seq_len, head_dim)\n",
        "        if self_k is not None and getattr(self_k, 'ndim', 0) >= 3:\n",
        "            self_k = self_k[:, :, -window:, :]\n",
        "        if self_v is not None and getattr(self_v, 'ndim', 0) >= 3:\n",
        "            self_v = self_v[:, :, -window:, :]\n",
        "\n",
        "        new_past.append((self_k, self_v) + rest)\n",
        "\n",
        "    return tuple(new_past)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_greedy_with_optional_window(model, tokenizer, sources, *, max_new_tokens=128, window=None):\n",
        "    \"\"\"Greedy decoding; if `window` is set, limit decoder self-attn KV history.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    for src in tqdm(list(sources), desc=f'generate(greedy, window={window})', leave=False):\n",
        "        enc = tokenizer(\n",
        "            'simplify: ' + str(src),\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt',\n",
        "        ).to(device)\n",
        "\n",
        "        # Compute adapted encoder outputs once\n",
        "        enc_out = model.base.encoder(\n",
        "            input_ids=enc['input_ids'],\n",
        "            attention_mask=enc['attention_mask'],\n",
        "            return_dict=True,\n",
        "        )\n",
        "        adapted = model.adapter(enc_out.last_hidden_state)\n",
        "        encoder_outputs = BaseModelOutput(last_hidden_state=adapted)\n",
        "\n",
        "        # T5 uses pad_token_id as decoder start token\n",
        "        generated = torch.tensor([[tokenizer.pad_token_id]], device=device)\n",
        "        past = None\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            if past is None:\n",
        "                decoder_input_ids = generated\n",
        "            else:\n",
        "                decoder_input_ids = generated[:, -1:]\n",
        "\n",
        "            out = model.base(\n",
        "                encoder_outputs=encoder_outputs,\n",
        "                attention_mask=enc['attention_mask'],\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                use_cache=True,\n",
        "                past_key_values=past,\n",
        "                return_dict=True,\n",
        "            )\n",
        "\n",
        "            next_token = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "            past = out.past_key_values\n",
        "            past = _truncate_t5_self_kv_past(past, window)\n",
        "\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        preds.append(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
        "\n",
        "    t1 = time.perf_counter()\n",
        "    ms_per_example = (t1 - t0) * 1000.0 / max(1, len(preds))\n",
        "    return preds, ms_per_example\n",
        "\n",
        "\n",
        "def sari_components_for_preds(preds, df):\n",
        "    sari_list, keep_list, del_list, add_list = [], [], [], []\n",
        "    for pred, src, ref in zip(preds, df['Normal'], df['Simple']):\n",
        "        s, comp = sari_score(str(src), str(pred), [str(ref)])\n",
        "        sari_list.append(s)\n",
        "        keep_list.append(comp['keep'])\n",
        "        del_list.append(comp['delete'])\n",
        "        add_list.append(comp['add'])\n",
        "    return (np.array(sari_list), np.array(keep_list), np.array(del_list), np.array(add_list))\n",
        "\n",
        "\n",
        "# 1) Existing “normal” dev_preds (beam search)\n",
        "beam_sari, beam_keep, beam_del, beam_add = sari_components_for_preds(dev_preds, dev_df)\n",
        "\n",
        "# 2) Greedy full context (window=None)\n",
        "greedy_full_preds, greedy_full_ms = generate_greedy_with_optional_window(\n",
        "    best_model, best_tokenizer, dev_df['Normal'], max_new_tokens=max_length, window=None\n",
        ")\n",
        "greedy_full_sari, greedy_full_keep, greedy_full_del, greedy_full_add = sari_components_for_preds(greedy_full_preds, dev_df)\n",
        "\n",
        "# 3) Greedy limited decoder self-attn context (e.g., last 64 tokens)\n",
        "window_tokens = 64\n",
        "limited_preds, limited_ms = generate_greedy_with_optional_window(\n",
        "    best_model, best_tokenizer, dev_df['Normal'], max_new_tokens=max_length, window=window_tokens\n",
        ")\n",
        "limited_sari, limited_keep, limited_del, limited_add = sari_components_for_preds(limited_preds, dev_df)\n",
        "\n",
        "\n",
        "# Summary table\n",
        "summary = pd.DataFrame([\n",
        "    {\n",
        "        'method': f'beam (normal, num_beams={num_beams})',\n",
        "        'SARI': beam_sari.mean(),\n",
        "        'KEEP': beam_keep.mean(),\n",
        "        'DELETE': beam_del.mean(),\n",
        "        'ADD': beam_add.mean(),\n",
        "        'ms/example (measured)': np.nan,\n",
        "    },\n",
        "    {\n",
        "        'method': 'greedy (full context)',\n",
        "        'SARI': greedy_full_sari.mean(),\n",
        "        'KEEP': greedy_full_keep.mean(),\n",
        "        'DELETE': greedy_full_del.mean(),\n",
        "        'ADD': greedy_full_add.mean(),\n",
        "        'ms/example (measured)': greedy_full_ms,\n",
        "    },\n",
        "    {\n",
        "        'method': f'greedy (decoder window={window_tokens})',\n",
        "        'SARI': limited_sari.mean(),\n",
        "        'KEEP': limited_keep.mean(),\n",
        "        'DELETE': limited_del.mean(),\n",
        "        'ADD': limited_add.mean(),\n",
        "        'ms/example (measured)': limited_ms,\n",
        "    },\n",
        "]).set_index('method')\n",
        "\n",
        "summary\n",
        "\n",
        "\n",
        "# Plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Mean SARI bar\n",
        "summary['SARI'].plot(kind='bar', ax=axes[0], title='DEV mean SARI')\n",
        "axes[0].set_ylabel('SARI')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Distribution comparison: greedy full vs limited\n",
        "axes[1].hist(greedy_full_sari, bins=30, alpha=0.6, label='greedy full')\n",
        "axes[1].hist(limited_sari, bins=30, alpha=0.6, label=f'greedy window={window_tokens}')\n",
        "axes[1].set_title('Per-example SARI distribution (greedy)')\n",
        "axes[1].set_xlabel('SARI')\n",
        "axes[1].set_ylabel('count')\n",
        "axes[1].legend()\n",
        "\n",
        "# Delta plot (limited - full) on greedy\n",
        "delta = limited_sari - greedy_full_sari\n",
        "axes[2].hist(delta, bins=30, alpha=0.8)\n",
        "axes[2].axvline(0, color='k', linewidth=1)\n",
        "axes[2].set_title('ΔSARI = windowed - full (greedy)')\n",
        "axes[2].set_xlabel('ΔSARI')\n",
        "axes[2].set_ylabel('count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# A small per-example table (top changes)\n",
        "per_example = pd.DataFrame({\n",
        "    'Normal': dev_df['Normal'],\n",
        "    'Reference': dev_df['Simple'],\n",
        "    'beam_pred': dev_preds,\n",
        "    'greedy_full_pred': greedy_full_preds,\n",
        "    'greedy_window_pred': limited_preds,\n",
        "    'greedy_full_SARI': greedy_full_sari,\n",
        "    'greedy_window_SARI': limited_sari,\n",
        "    'delta_SARI(window-full)': delta,\n",
        "})\n",
        "\n",
        "per_example.sort_values('delta_SARI(window-full)', ascending=True).head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate outputs on test.csv and write to a file (one simplification per line)\n",
        "test_preds = generate_text(best_model, best_tokenizer, test_df['Normal'])\n",
        "out_path = 'outputs_extension3.txt'\n",
        "with open(out_path, 'w', encoding='utf-8') as f:\n",
        "    for p in test_preds:\n",
        "        f.write(p.strip() + '\\n')\n",
        "print('wrote:', out_path, 'num_lines:', len(test_preds))\n",
        "\n",
        "# Optional: evaluate on this repo's test.csv (it includes Simple references here)\n",
        "sari_scores = []\n",
        "for pred, src, ref in zip(test_preds, test_df['Normal'], test_df['Simple']):\n",
        "    s, _ = sari_score(str(src), str(pred), [str(ref)])\n",
        "    sari_scores.append(s)\n",
        "print('TEST SARI (local, if references exist):', float(np.mean(sari_scores)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zip the trained model directory for easy submission/sharing\n",
        "import shutil\n",
        "zip_base = 't5-simplification-2'\n",
        "zip_path = shutil.make_archive(zip_base, 'zip', root_dir=output_dir)\n",
        "zip_path\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
