{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "64XjuioTwCJb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Tuple\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text: str) -> List[str]:\n",
        "    return text.lower().split()\n",
        "\n",
        "\n",
        "def get_ngrams(tokens: List[str], n: int) -> Counter:\n",
        "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1))\n",
        "\n",
        "\n",
        "def sari_score(source: str, prediction: str, references: List[str]) -> Tuple[float, dict]:\n",
        "    src_tokens = tokenize(source)\n",
        "    pred_tokens = tokenize(prediction)\n",
        "    ref_tokens_list = [tokenize(ref) for ref in references]\n",
        "\n",
        "    keep_scores = []\n",
        "    del_scores = []\n",
        "    add_scores = []\n",
        "\n",
        "    for n in range(1, 5):\n",
        "        src_ngrams = get_ngrams(src_tokens, n)\n",
        "        pred_ngrams = get_ngrams(pred_tokens, n)\n",
        "        ref_ngrams_list = [get_ngrams(ref_tokens, n) for ref_tokens in ref_tokens_list]\n",
        "\n",
        "        src_in_refs = Counter()\n",
        "        for ngram in src_ngrams:\n",
        "            count = sum(1 for ref_ngrams in ref_ngrams_list if ngram in ref_ngrams)\n",
        "            if count > 0:\n",
        "                src_in_refs[ngram] = count\n",
        "\n",
        "        kept_ngrams = Counter()\n",
        "        for ngram in pred_ngrams:\n",
        "            if ngram in src_ngrams:\n",
        "                kept_ngrams[ngram] = min(pred_ngrams[ngram], src_ngrams[ngram])\n",
        "\n",
        "        keep_prec_num = sum(min(kept_ngrams[ng], 1) for ng in kept_ngrams if ng in src_in_refs)\n",
        "        keep_prec_den = sum(kept_ngrams.values())\n",
        "        keep_prec = keep_prec_num / keep_prec_den if keep_prec_den > 0 else 0\n",
        "\n",
        "        keep_rec_num = sum(min(kept_ngrams[ng], 1) for ng in src_in_refs if ng in kept_ngrams)\n",
        "        keep_rec_den = len(src_in_refs)\n",
        "        keep_rec = keep_rec_num / keep_rec_den if keep_rec_den > 0 else 0\n",
        "\n",
        "        keep_f1 = 2 * keep_prec * keep_rec / (keep_prec + keep_rec) if (keep_prec + keep_rec) > 0 else 0\n",
        "        keep_scores.append(keep_f1)\n",
        "\n",
        "        src_not_in_refs = set(ng for ng in src_ngrams if ng not in src_in_refs)\n",
        "        deleted_ngrams = set(ng for ng in src_ngrams if ng not in pred_ngrams)\n",
        "\n",
        "        del_prec_num = len(deleted_ngrams & src_not_in_refs)\n",
        "        del_prec_den = len(deleted_ngrams)\n",
        "        del_prec = del_prec_num / del_prec_den if del_prec_den > 0 else 0\n",
        "\n",
        "        del_rec_num = len(deleted_ngrams & src_not_in_refs)\n",
        "        del_rec_den = len(src_not_in_refs)\n",
        "        del_rec = del_rec_num / del_rec_den if del_rec_den > 0 else 0\n",
        "\n",
        "        del_f1 = 2 * del_prec * del_rec / (del_prec + del_rec) if (del_prec + del_rec) > 0 else 0\n",
        "        del_scores.append(del_f1)\n",
        "\n",
        "        added_ngrams = set(ng for ng in pred_ngrams if ng not in src_ngrams)\n",
        "\n",
        "        ref_not_in_src = set()\n",
        "        for ref_ngrams in ref_ngrams_list:\n",
        "            ref_not_in_src.update(ng for ng in ref_ngrams if ng not in src_ngrams)\n",
        "\n",
        "        add_prec_num = len(added_ngrams & ref_not_in_src)\n",
        "        add_prec_den = len(added_ngrams)\n",
        "        add_prec = add_prec_num / add_prec_den if add_prec_den > 0 else 0\n",
        "\n",
        "        add_scores.append(add_prec)\n",
        "\n",
        "    keep_avg = sum(keep_scores) / 4\n",
        "    del_avg = sum(del_scores) / 4\n",
        "    add_avg = sum(add_scores) / 4\n",
        "\n",
        "    sari = (keep_avg + del_avg + add_avg) / 3 * 100\n",
        "\n",
        "    components = {\n",
        "        'keep': keep_avg * 100,\n",
        "        'delete': del_avg * 100,\n",
        "        'add': add_avg * 100\n",
        "    }\n",
        "\n",
        "    return sari, components"
      ],
      "metadata": {
        "id": "dqR2iIVdwneB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, sources: List[str], targets: List[str], tokenizer, max_length: int = 128):\n",
        "        self.sources = sources\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sources)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source = \"simplify: \" + self.sources[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        source_enc = self.tokenizer(\n",
        "            source,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        target_enc = self.tokenizer(\n",
        "            target,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        labels = target_enc['input_ids'].squeeze()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': source_enc['input_ids'].squeeze(),\n",
        "            'attention_mask': source_enc['attention_mask'].squeeze(),\n",
        "            'labels': labels\n",
        "        }"
      ],
      "metadata": {
        "id": "r0EMqk6JwLq_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_sources: List[str],\n",
        "    train_targets: List[str],\n",
        "    val_sources: List[str],\n",
        "    val_targets: List[str],\n",
        "    output_dir: str,\n",
        "    epochs: int = 3,\n",
        "    batch_size: int = 16,\n",
        "    learning_rate: float = 3e-5,\n",
        "    device: str = 'cuda'\n",
        "):\n",
        "    train_dataset = SimplificationDataset(train_sources, train_targets, tokenizer)\n",
        "    val_dataset = SimplificationDataset(val_sources, val_targets, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                val_loss += outputs.loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            model.save_pretrained(output_dir)\n",
        "            tokenizer.save_pretrained(output_dir)\n",
        "            print(f\"Saved best model to {output_dir}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "V7YhfMDJwOpP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strong_baseline(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    max_length: int = 128,\n",
        "    num_beams: int = 4,\n",
        "    device: str = 'cuda'\n",
        ") -> str:\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_text = \"simplify: \" + source\n",
        "        input_enc = tokenizer(\n",
        "            input_text,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_enc['input_ids'],\n",
        "            attention_mask=input_enc['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "KM2okXDlwWUq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 't5-small'\n",
        "output_dir = './t5-simplification'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 20\n",
        "batch_size = 16\n",
        "learning_rate = 3e-5\n",
        "\n",
        "splits = {\n",
        "    'train': 'wiki.full.aner.ori.train.95.tsv',\n",
        "    'validation': 'wiki.full.aner.ori.valid.95.tsv',\n",
        "    'test': 'wiki.full.aner.ori.test.95.tsv'\n",
        "}\n",
        "\n",
        "print(\"Loading WikiLarge dataset...\")\n",
        "train = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"train\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "val = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"validation\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "test = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"test\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "model = train_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_sources=train['Normal'].tolist(),\n",
        "    train_targets=train['Simple'].tolist(),\n",
        "    val_sources=val['Normal'].tolist(),\n",
        "    val_targets=val['Simple'].tolist(),\n",
        "    output_dir=output_dir,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "outputs = []\n",
        "for source in tqdm(test['Normal'], desc=\"Generating simplifications\"):\n",
        "    output = strong_baseline(model, tokenizer, source, device=device)\n",
        "    outputs.append(output)\n",
        "\n",
        "\n",
        "sari_scores = []\n",
        "keep_scores = []\n",
        "del_scores = []\n",
        "add_scores = []\n",
        "\n",
        "for output, source, reference in zip(outputs, test['Normal'], test['Simple']):\n",
        "    sari, components = sari_score(source, output, [reference])\n",
        "    sari_scores.append(sari)\n",
        "    keep_scores.append(components['keep'])\n",
        "    del_scores.append(components['delete'])\n",
        "    add_scores.append(components['add'])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Strong Baseline SARI Score Results\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Number of samples: {len(sari_scores)}\")\n",
        "print()\n",
        "print(f\"  SARI:        {sum(sari_scores) / len(sari_scores):.2f}\")\n",
        "print(f\"    - Keep:    {sum(keep_scores) / len(keep_scores):.2f}\")\n",
        "print(f\"    - Delete:  {sum(del_scores) / len(del_scores):.2f}\")\n",
        "print(f\"    - Add:     {sum(add_scores) / len(add_scores):.2f}\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6nvsoCwwXGV",
        "outputId": "911d7348-8d41-4a95-c77d-35347f93595a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WikiLarge dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 9303/9303 [12:01<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 1.7627, Val Loss = 1.4882\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss = 1.5156, Val Loss = 1.4143\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 9303/9303 [11:59<00:00, 12.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss = 1.4465, Val Loss = 1.3770\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 9303/9303 [12:05<00:00, 12.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss = 1.4079, Val Loss = 1.3585\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 9303/9303 [11:44<00:00, 13.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss = 1.3829, Val Loss = 1.3476\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss = 1.3632, Val Loss = 1.3366\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss = 1.3484, Val Loss = 1.3273\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss = 1.3346, Val Loss = 1.3245\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss = 1.3238, Val Loss = 1.3166\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 9303/9303 [12:02<00:00, 12.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss = 1.3141, Val Loss = 1.3140\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 9303/9303 [11:44<00:00, 13.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train Loss = 1.3058, Val Loss = 1.3094\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train Loss = 1.2981, Val Loss = 1.3060\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 9303/9303 [11:58<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train Loss = 1.2920, Val Loss = 1.3048\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 9303/9303 [11:58<00:00, 12.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train Loss = 1.2872, Val Loss = 1.3035\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 9303/9303 [11:57<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train Loss = 1.2822, Val Loss = 1.3029\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 9303/9303 [11:59<00:00, 12.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Train Loss = 1.2774, Val Loss = 1.3003\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 9303/9303 [11:45<00:00, 13.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Train Loss = 1.2746, Val Loss = 1.2992\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 9303/9303 [11:59<00:00, 12.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Train Loss = 1.2715, Val Loss = 1.2991\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 9303/9303 [11:59<00:00, 12.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Train Loss = 1.2699, Val Loss = 1.2992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 9303/9303 [11:58<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train Loss = 1.2688, Val Loss = 1.2985\n",
            "Saved best model to ./t5-simplification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating simplifications: 100%|██████████| 191/191 [01:03<00:00,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Strong Baseline SARI Score Results\n",
            "==================================================\n",
            "Number of samples: 191\n",
            "\n",
            "  SARI:        33.43\n",
            "    - Keep:    59.90\n",
            "    - Delete:  31.98\n",
            "    - Add:     8.41\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r t5-simplification.zip /content/t5-simplification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXo8Mp_fo8yZ",
        "outputId": "c728e9bd-70f4-4e2d-ec89-3bf5b176a5b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: /t5-simplification\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r t5-simplification.zip . -i /t5-simplification)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('t5-simplification.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KLfzLGIgpBN3",
        "outputId": "98ee885a-2256-41ff-969d-8cf4babbeb12"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d3ab1e98-e594-425e-9438-2b1342e2254d\", \"t5-simplification.zip\", 224818645)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aec905c"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('t5-simplification.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_dir = 'content/t5-simplification'\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = T5ForConditionalGeneration.from_pretrained(output_dir)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "# model.to(device)\n",
        "\n",
        "# test = pd.read_csv(\n",
        "#     \"hf://datasets/bogdancazan/wikilarge-text-simplification/wiki.full.aner.ori.test.95.tsv\",\n",
        "#     sep=\"\\t\"\n",
        "# )\n",
        "\n",
        "# print(\"Generating predictions and calculating scores...\")\n",
        "# results = []\n",
        "# for idx, row in tqdm(test.iterrows(), total=len(test)):\n",
        "#     source = row['Normal']\n",
        "#     reference = row['Simple']\n",
        "\n",
        "#     output = strong_baseline(model, tokenizer, source, device=device)\n",
        "\n",
        "#     sari, components = sari_score(source, output, [reference])\n",
        "\n",
        "#     results.append({\n",
        "#         'idx': idx,\n",
        "#         'source': source,\n",
        "#         'reference': reference,\n",
        "#         'prediction': output,\n",
        "#         'sari': sari,\n",
        "#         'keep': components['keep'],\n",
        "#         'delete': components['delete'],\n",
        "#         'add': components['add']\n",
        "#     })\n",
        "\n",
        "# results_df = pd.DataFrame(results)\n",
        "\n",
        "# results_df = results_df.sort_values('sari', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*80)\n",
        "# print(\"TOP 5 EXAMPLES (Highest SARI Scores)\")\n",
        "# print(\"=\"*80)\n",
        "# for i, row in results_df.head(5).iterrows():\n",
        "#     print(f\"\\nExample {i+1} - SARI: {row['sari']:.2f} (Keep: {row['keep']:.2f}, Del: {row['delete']:.2f}, Add: {row['add']:.2f})\")\n",
        "#     print(f\"Source:     {row['source']}\")\n",
        "#     print(f\"Reference:  {row['reference']}\")\n",
        "#     print(f\"Prediction: {row['prediction']}\")\n",
        "#     print(\"-\"*80)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*80)\n",
        "# print(\"BOTTOM 5 EXAMPLES (Lowest SARI Scores)\")\n",
        "# print(\"=\"*80)\n",
        "# for i, row in results_df.tail(5).iterrows():\n",
        "#     print(f\"\\nExample {i+1} - SARI: {row['sari']:.2f} (Keep: {row['keep']:.2f}, Del: {row['delete']:.2f}, Add: {row['add']:.2f})\")\n",
        "#     print(f\"Source:     {row['source']}\")\n",
        "#     print(f\"Reference:  {row['reference']}\")\n",
        "#     print(f\"Prediction: {row['prediction']}\")\n",
        "#     print(\"-\"*80)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*80)\n",
        "# print(\"SUMMARY STATISTICS\")\n",
        "# print(\"=\"*80)\n",
        "# print(f\"Total examples: {len(results_df)}\")\n",
        "# print(f\"\\nSARI Score Distribution:\")\n",
        "# print(f\"  Mean:   {results_df['sari'].mean():.2f}\")\n",
        "# print(f\"  Median: {results_df['sari'].median():.2f}\")\n",
        "# print(f\"  Std:    {results_df['sari'].std():.2f}\")\n",
        "# print(f\"  Min:    {results_df['sari'].min():.2f}\")\n",
        "# print(f\"  Max:    {results_df['sari'].max():.2f}\")\n",
        "# print(\"=\"*80)\n",
        "\n",
        "# results_df.to_csv('model_analysis_results.csv', index=False)\n",
        "# print(f\"\\nResults saved to 'model_analysis_results.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKZrQw59ozY-",
        "outputId": "fd0517de-6c72-40df-b718-b22996effdfb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions and calculating scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 191/191 [01:01<00:00,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TOP 5 EXAMPLES (Highest SARI Scores)\n",
            "================================================================================\n",
            "\n",
            "Example 1 - SARI: 100.00 (Keep: 100.00, Del: 100.00, Add: 100.00)\n",
            "Source:     a majority of south indians speak one of the five dravidian languages kannada malayalam tamil telugu and tulu.\n",
            "Reference:  most south indians speak one of the five dravidian languages kannada malayalam tamil telugu and tulu.\n",
            "Prediction: most south indians speak one of the five dravidian languages kannada malayalam tamil telugu and tulu.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 2 - SARI: 99.60 (Keep: 98.81, Del: 100.00, Add: 100.00)\n",
            "Source:     the primavera is a painting by the italian renaissance painter sandro botticelli c..\n",
            "Reference:  painted around the primavera is a painting by the italian renaissance painter sandro botticelli.\n",
            "Prediction: the primavera is a painting by the italian renaissance painter sandro botticelli.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 3 - SARI: 90.78 (Keep: 92.82, Del: 79.52, Add: 100.00)\n",
            "Source:     shortly after attaining category status the outer convection of the hurricane became ragged.\n",
            "Reference:  shortly after reaching category status the outer convection of the hurricane became worn out\n",
            "Prediction: shortly after reaching category status the outer convection of the hurricane became ragged.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 4 - SARI: 90.12 (Keep: 83.14, Del: 87.22, Add: 100.00)\n",
            "Source:     in culver ran for iowa secretary of state and was victorious.\n",
            "Reference:  in culver successfully ran for iowa secretary of state.\n",
            "Prediction: in culver ran for iowa secretary of state.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 5 - SARI: 86.77 (Keep: 77.70, Del: 82.60, Add: 100.00)\n",
            "Source:     napoleonic wars austrian general mack surrenders his army to the grand army of napoleon at ulm reaping napoleon over prisoners and inflicting casualties on the losers.\n",
            "Reference:  austrian general mack surrenders his army to grand army of napoleon at ulm.\n",
            "Prediction: napoleonic wars austrian general mack surrenders his army to the grand army of napoleon at ulm.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "BOTTOM 5 EXAMPLES (Lowest SARI Scores)\n",
            "================================================================================\n",
            "\n",
            "Example 187 - SARI: 9.68 (Keep: 29.05, Del: 0.00, Add: 0.00)\n",
            "Source:     the latter device can then be designed and used in less stringent environments.\n",
            "Reference:  the device can be designed for use in less exact environments.\n",
            "Prediction: the latter device can then be designed and used in less stringent environments.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 188 - SARI: 9.23 (Keep: 27.69, Del: 0.00, Add: 0.00)\n",
            "Source:     terms such as undies for underwear and movie for moving picture are oft heard terms in english.\n",
            "Reference:  words like undies movie are oft heard terms in english.\n",
            "Prediction: terms such as undies for underwear and movie for moving picture are oft heard in english.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 189 - SARI: 7.24 (Keep: 21.72, Del: 0.00, Add: 0.00)\n",
            "Source:     defiantly she vowed to never renounce the commune and dared the judges to sentence her to death.\n",
            "Reference:  she refused to give up the commune and prefered the death sentence\n",
            "Prediction: defiantly she vowed to never renounce the commune and dared the judges to sentence her to death.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 190 - SARI: 6.75 (Keep: 20.24, Del: 0.00, Add: 0.00)\n",
            "Source:     therefore these pdfs can not be distributed without further manipulation if they contain images.\n",
            "Reference:  if any of these pdfs contain pictures then they require additional processing before they can be issued\n",
            "Prediction: therefore these pdfs can not be distributed without further manipulation if they contain images.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 191 - SARI: 2.78 (Keep: 8.33, Del: 0.00, Add: 0.00)\n",
            "Source:     characteristics radar observations indicate a fairly pure iron nickel composition.\n",
            "Reference:  radar testing shows composition of mostly iron nickel.\n",
            "Prediction: characteristics radar observations indicate a fairly pure iron nickel composition.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Total examples: 191\n",
            "\n",
            "SARI Score Distribution:\n",
            "  Mean:   33.43\n",
            "  Median: 28.12\n",
            "  Std:    18.10\n",
            "  Min:    2.78\n",
            "  Max:    100.00\n",
            "================================================================================\n",
            "\n",
            "Results saved to 'model_analysis_results.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_prefix(source: str, completion_ratio: float) -> str:\n",
        "    tokens = source.split()\n",
        "    num_tokens = len(tokens)\n",
        "    prefix_length = max(1, int(num_tokens * completion_ratio))\n",
        "\n",
        "    prefix_tokens = tokens[:prefix_length]\n",
        "    return ' '.join(prefix_tokens)\n",
        "\n",
        "\n",
        "def incremental_simplify(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    completion_ratio: float,\n",
        "    max_length: int = 128,\n",
        "    num_beams: int = 4,\n",
        "    length_penalty: float = 0.6,\n",
        "    device: str = 'cuda'\n",
        ") -> str:\n",
        "    prefix = get_sentence_prefix(source, completion_ratio)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_text = \"simplify: \" + prefix\n",
        "        input_enc = tokenizer(\n",
        "            input_text,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_enc['input_ids'],\n",
        "            attention_mask=input_enc['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=length_penalty,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "\n",
        "def adaptive_incremental_simplify(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    source: str,\n",
        "    completion_ratio: float,\n",
        "    max_length: int = 128,\n",
        "    num_beams: int = 4,\n",
        "    device: str = 'cuda'\n",
        ") -> str:\n",
        "    if completion_ratio <= 0.25:\n",
        "        length_penalty = 0.4\n",
        "    elif completion_ratio <= 0.5:\n",
        "        length_penalty = 0.5\n",
        "    elif completion_ratio <= 0.75:\n",
        "        length_penalty = 0.6\n",
        "    else:\n",
        "        length_penalty = 0.7\n",
        "\n",
        "    return incremental_simplify(\n",
        "        model, tokenizer, source, completion_ratio,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=length_penalty,\n",
        "        device=device\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "Wdu1bSc33rN5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = 'content/t5-simplification'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_length = 128\n",
        "\n",
        "model_dir = os.path.abspath(model_dir)\n",
        "\n",
        "splits = {\n",
        "    'test': 'wiki.full.aner.ori.test.95.tsv'\n",
        "}\n",
        "\n",
        "print(\"Loading fine-tuned T5 model...\")\n",
        "if not os.path.exists(model_dir):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Model directory {model_dir} not found. \"\n",
        "        \"Please run strong_baseline.py first to train the model.\"\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir, local_files_only=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "print(\"Loading WikiLarge test dataset...\")\n",
        "test = pd.read_csv(\n",
        "    \"hf://datasets/bogdancazan/wikilarge-text-simplification/\" + splits[\"test\"],\n",
        "    sep=\"\\t\"\n",
        ")\n",
        "print(f\"Loaded {len(test)} test examples\")\n",
        "\n",
        "completion_ratios = [0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "print(\"\\nEvaluating incremental simplification with fixed length penalty...\")\n",
        "fixed_results = {ratio: {'sari': [], 'keep': [], 'delete': [], 'add': []} for ratio in completion_ratios}\n",
        "\n",
        "for idx, row in tqdm(test.iterrows(), total=len(test), desc=\"Processing\"):\n",
        "    source = row['Normal']\n",
        "    reference = row['Simple']\n",
        "\n",
        "    for ratio in completion_ratios:\n",
        "        output = incremental_simplify(\n",
        "            model, tokenizer, source, ratio,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            length_penalty=0.6,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        sari, components = sari_score(source, output, [reference])\n",
        "        fixed_results[ratio]['sari'].append(sari)\n",
        "        fixed_results[ratio]['keep'].append(components['keep'])\n",
        "        fixed_results[ratio]['delete'].append(components['delete'])\n",
        "        fixed_results[ratio]['add'].append(components['add'])\n",
        "\n",
        "print(\"\\nEvaluating incremental simplification with adaptive length penalty...\")\n",
        "adaptive_results = {ratio: {'sari': [], 'keep': [], 'delete': [], 'add': []} for ratio in completion_ratios}\n",
        "\n",
        "for idx, row in tqdm(test.iterrows(), total=len(test), desc=\"Processing\"):\n",
        "    source = row['Normal']\n",
        "    reference = row['Simple']\n",
        "\n",
        "    for ratio in completion_ratios:\n",
        "        output = adaptive_incremental_simplify(\n",
        "            model, tokenizer, source, ratio,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        sari, components = sari_score(source, output, [reference])\n",
        "        adaptive_results[ratio]['sari'].append(sari)\n",
        "        adaptive_results[ratio]['keep'].append(components['keep'])\n",
        "        adaptive_results[ratio]['delete'].append(components['delete'])\n",
        "        adaptive_results[ratio]['add'].append(components['add'])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Fixed Length Penalty Results\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n{'Completion Ratio':<20} {'SARI':<10} {'Keep':<10} {'Delete':<10} {'Add':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    sari_avg = np.mean(fixed_results[ratio]['sari'])\n",
        "    keep_avg = np.mean(fixed_results[ratio]['keep'])\n",
        "    del_avg = np.mean(fixed_results[ratio]['delete'])\n",
        "    add_avg = np.mean(fixed_results[ratio]['add'])\n",
        "\n",
        "    print(f\"{ratio*100:>5.0f}%{'':<14} {sari_avg:>6.2f}    {keep_avg:>6.2f}    {del_avg:>6.2f}    {add_avg:>6.2f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Adaptive Length Penalty Results\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n{'Completion Ratio':<20} {'SARI':<10} {'Keep':<10} {'Delete':<10} {'Add':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    sari_avg = np.mean(adaptive_results[ratio]['sari'])\n",
        "    keep_avg = np.mean(adaptive_results[ratio]['keep'])\n",
        "    del_avg = np.mean(adaptive_results[ratio]['delete'])\n",
        "    add_avg = np.mean(adaptive_results[ratio]['add'])\n",
        "\n",
        "    print(f\"{ratio*100:>5.0f}%{'':<14} {sari_avg:>6.2f}    {keep_avg:>6.2f}    {del_avg:>6.2f}    {add_avg:>6.2f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Comparison: Fixed vs Adaptive Length Penalty\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n{'Completion Ratio':<20} {'Fixed SARI':<15} {'Adaptive SARI':<15} {'Improvement':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    fixed_sari = np.mean(fixed_results[ratio]['sari'])\n",
        "    adaptive_sari = np.mean(adaptive_results[ratio]['sari'])\n",
        "    improvement = adaptive_sari - fixed_sari\n",
        "\n",
        "    print(f\"{ratio*100:>5.0f}%{'':<14} {fixed_sari:>8.2f}      {adaptive_sari:>8.2f}      {improvement:>+8.2f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "full_context_sari = np.mean(adaptive_results[1.0]['sari'])\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Quality Degradation Analysis (Adaptive Strategy)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nFull context (100%) SARI: {full_context_sari:.2f}\")\n",
        "print(f\"\\n{'Completion Ratio':<20} {'SARI':<15} {'% of Full Quality':<20}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    sari = np.mean(adaptive_results[ratio]['sari'])\n",
        "    pct_quality = (sari / full_context_sari) * 100 if full_context_sari > 0 else 0\n",
        "    print(f\"{ratio*100:>5.0f}%{'':<14} {sari:>8.2f}      {pct_quality:>15.1f}%\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Example: Incremental Simplification\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "example_idx = 0\n",
        "source = test.iloc[example_idx]['Normal']\n",
        "reference = test.iloc[example_idx]['Simple']\n",
        "\n",
        "print(f\"\\nSource: {source}\")\n",
        "print(f\"Reference: {reference}\")\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "\n",
        "for ratio in completion_ratios:\n",
        "    prefix = get_sentence_prefix(source, ratio)\n",
        "    output = adaptive_incremental_simplify(\n",
        "        model, tokenizer, source, ratio,\n",
        "        max_length=max_length,\n",
        "        num_beams=4,\n",
        "        device=device\n",
        "    )\n",
        "    sari, _ = sari_score(source, output, [reference])\n",
        "\n",
        "    print(f\"\\n[{ratio*100:.0f}% context]\")\n",
        "    print(f\"  Prefix: {prefix}\")\n",
        "    print(f\"  Output: {output}\")\n",
        "    print(f\"  SARI: {sari:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GToebvC_4Ra-",
        "outputId": "03ef78ef-d808-47bf-fcd5-760930c491c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fine-tuned T5 model...\n",
            "Model loaded successfully!\n",
            "Loading WikiLarge test dataset...\n",
            "Loaded 191 test examples\n",
            "\n",
            "Evaluating incremental simplification with fixed length penalty...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 191/191 [03:04<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating incremental simplification with adaptive length penalty...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 191/191 [03:01<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Fixed Length Penalty Results\n",
            "======================================================================\n",
            "\n",
            "Completion Ratio     SARI       Keep       Delete     Add       \n",
            "----------------------------------------------------------------------\n",
            "   25%                28.74     22.90     61.76      1.56\n",
            "   50%                32.25     39.16     54.19      3.39\n",
            "   75%                33.92     47.98     48.95      4.81\n",
            "  100%                36.01     58.18     40.10      9.75\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Adaptive Length Penalty Results\n",
            "======================================================================\n",
            "\n",
            "Completion Ratio     SARI       Keep       Delete     Add       \n",
            "----------------------------------------------------------------------\n",
            "   25%                28.74     22.90     61.76      1.56\n",
            "   50%                32.27     39.14     54.28      3.39\n",
            "   75%                33.92     47.98     48.95      4.81\n",
            "  100%                35.94     58.44     39.58      9.81\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Comparison: Fixed vs Adaptive Length Penalty\n",
            "======================================================================\n",
            "\n",
            "Completion Ratio     Fixed SARI      Adaptive SARI   Improvement    \n",
            "----------------------------------------------------------------------\n",
            "   25%                  28.74         28.74         +0.00\n",
            "   50%                  32.25         32.27         +0.02\n",
            "   75%                  33.92         33.92         +0.00\n",
            "  100%                  36.01         35.94         -0.07\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Quality Degradation Analysis (Adaptive Strategy)\n",
            "======================================================================\n",
            "\n",
            "Full context (100%) SARI: 35.94\n",
            "\n",
            "Completion Ratio     SARI            % of Full Quality   \n",
            "----------------------------------------------------------------------\n",
            "   25%                  28.74                 80.0%\n",
            "   50%                  32.27                 89.8%\n",
            "   75%                  33.92                 94.4%\n",
            "  100%                  35.94                100.0%\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Example: Incremental Simplification\n",
            "======================================================================\n",
            "\n",
            "Source: his next work saturday follows an especially eventful day in the life of a successful neurosurgeon.\n",
            "Reference: his next work at saturday will be a successful neurosurgeon.\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[25% context]\n",
            "  Prefix: his next work saturday\n",
            "  Output: his next work saturday\n",
            "  SARI: 45.02\n",
            "\n",
            "[50% context]\n",
            "  Prefix: his next work saturday follows an especially eventful\n",
            "  Output: his next work saturday follows an especially eventful\n",
            "  SARI: 31.91\n",
            "\n",
            "[75% context]\n",
            "  Prefix: his next work saturday follows an especially eventful day in the life\n",
            "  Output: his next work saturday follows an especially eventful day in the life\n",
            "  SARI: 17.67\n",
            "\n",
            "[100% context]\n",
            "  Prefix: his next work saturday follows an especially eventful day in the life of a successful neurosurgeon.\n",
            "  Output: his next work saturday follows an eventful day in the life of a successful neurosurgeon.\n",
            "  SARI: 23.27\n"
          ]
        }
      ]
    }
  ]
}